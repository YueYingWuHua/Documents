[TOC]

--------------------------
# kubernetes常用操作
## 部署时常用操作
```shell
  sudo systemctl stop xxx.service
  sudo cp xxx /etc/systemd/system/xxx.service
  systemctl daemon-reload
  sudo systemctl enable xxx.service
  sudo systemctl restart xxx.service
```

## 使用时常用操作
```shell
  kubectl get svc -n kube-system
  kubectl get svc --all-namespaces
  kubectl get nodes --all-namespaces
  kubectl create -f xxxx.yaml
  kubectl delete -f xxxx.yaml
  kubectl exec nginx -i -t -- /bin/bash
  kubectl cluster-info
  kubectl logs kubernetes-dashboard-5bc57d65cf-n8wrm -n kube-system
  #获取token用来登录
  kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}') kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')
```
# etcd
    生成key时要通过配置中的hosts或者在生成时指定--host，不然会导致证书不可用

    for ip in ${NODE_IPS}; do
    ETCDCTL_API=3 /kubernetes/local/bin/etcdctl \
    --endpoints=https://${ip}:2379 \
    --cacert=/etc/kubernetes/ssl/ca.pem \
    --cert=/etc/etcd/ssl/etcd.pem \
    --key=/etc/etcd/ssl/etcd-key.pem \
    endpoint health; done

# flannel
    ifconfig flannel.1
    ping 172.30.12.0
    ping 172.30.76.0
    ping 172.30.18.0

# kube-apiserver

已经没有token验证方式了，需要指定kubelet需要的验证文件
创建一个encryption-config.yaml，暂时不知道有啥用

kube-apiserver安装方式有多种，单节点：易于部署，但是容错差，一种replica，一种HA，对于云上安装可以不考虑HA直接采用google cloud的服务或者AWS的服务，如果物理机部署推荐采用replica方式。但是部署调试时建议使用单机模式。

api-server测试过程：

遇到的问题有：
1、journalctl -xe中如下日志刷一大堆，而且时时刻刻在刷；
```shell
    1月 07 14:40:16 cloud25 kube-apiserver[19798]: I0107 14:40:16.695016   19798 wrap.go:42] GET /apis/admissionregistration.k8s.io/v1alpha1/initializerconfigurations: (2.686028ms) 404 [[kube-apiserver/v1.10.2 (linux/amd64) kubernetes/81753b1
```
这个是dymanic admission control的问题,此项特性在目前的版本（1.10.2）下是测试特性，默认关闭需要手动开启，然而开启这个特性需要在启动的时候加入两个参数，如果并且需要指定下面两项（enable-admission-plugins中加入Initializers，并且--runtime-config加入admissionregistration.k8s.io/v1alpha1）：
```shell
 --enable-admission-plugins=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \
 --runtime-config=api/all,admissionregistration.k8s.io/v1alpha1 \
```

注：在多数手动安装教程中，--runtime-config=api/all \ 在实际安装中，可以先不加后面那项，如果遇到问题了再加上也不麻烦。

2、journalctl -xe中如下日志刷一大堆，而且时时刻刻在刷；
```shell
    1月 07 14:40:16 cloud25 kube-apiserver[19798]: I0107 14:40:16.695016   19798 wrap.go:42] GET /apis/admissionregistration.k8s.io/v1alpha1/initializerconfigurations: (2.686028ms) 200 [[kube-apiserver/v1.10.2 (linux/amd64) kubernetes/81753b1
```
看起来和上面一样，其实是404换成了200。这些日志应该没有太多影响，只要kubectl能正常获取系统配置就行。但是这2秒17条实在是太影响日志可读性了。journalctl -u kube-apiserver中的所有有用信息都被淹没在这些东西里了。

3、端口不对，apiserver指定的是6443端口。
看起来是一个小问题，然而这问题耽误了我半天时间。首先要说明的是，不论怎么安装，之前都要创建~/.kube/config文件，在创建这个文件时是通过kubectl进行写入的，而且往往这个文件配置的比较早，实际配置apiserver的时候大多又不会手动指定端口，如果配置.kube/config中的配置时指定的不是6443端口，那么就难受了。
系统会提示：
```shell
  kubectl cluster-info
  Kubernetes master is running at https://192.168.12.165:xxxx
  The connection to the server 192.168.12.165:xxxx was refused - did you specify the right host or port?
```
我看到这日志的第一反应是连接被拒绝，是不是证书过期？或者说启动出错？配合上面提到的第2个2秒17条的日志效果拔群。总以为是启动问题，其实“Kubernetes master is running at https://192.168.12.165:xxxx”这句话改成“connecting to kubernetes master: https://192.168.12.165:xxxx”可能比较容易想到是IP或者端口指定的问题。我是最后发现netstat -ano | grep 8443没启动，查stackoverflow时发现有人遇到类似的问题，最后通过修改./kube/config解决的，我才反应过来要查一下apiserver的默认端口，一查是6443然后发现已经监听好久了。

# docker
    需要在配置里指定PATH，不然找不到其他运行脚本

# kubelet
    记得把workspace路径提前创建好

# kubeDNS
## 修改kube-dns.yaml
    #先去github摸个完整源码回来
    git clone https://github.com/kubernetes/kubernetes.git
    #cd kubernetes/cluster/addons/dns
    mv kube-dns.yaml.base kube-dns.yaml
    vim kube-dns.yaml
    #修改一些乱七八糟东西
    clusterIP是准备好的DNS地址，image反正连不上，爱改成哪都行，反正原理就是拉个镜像回来，yueyingwuhua这个是我自己在dockerhub上摸的镜像。
    cloud@cloud25:~/k8s$ diff kube-dns.yaml.base kube-dns.yaml
    33c33
    <   clusterIP: __PILLAR__DNS__SERVER__
    ---
    >   clusterIP: 192.254.0.2
    98c98
    <         image: k8s.gcr.io/k8s-dns-kube-dns-amd64:1.14.10
    ---
    >         image: yueyingwuhua/k8s-dns-kube-dns-amd64:1.14.10
    128c128
    <         - --domain=__PILLAR__DNS__DOMAIN__.
    ---
    >         - --domain=cluster.local.
    149c149
    <         image: k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.14.10
    ---
    >         image: yueyingwuhua/k8s-dns-dnsmasq-nanny-amd64:1.14.10
    169c169
    <         - --server=/__PILLAR__DNS__DOMAIN__/127.0.0.1#10053
    ---
    >         - --server=/cluster.local/127.0.0.1#10053
    188c188
    <         image: k8s.gcr.io/k8s-dns-sidecar-amd64:1.14.10
    ---
    >         image: yueyingwuhua/k8s-dns-sidecar-amd64:1.14.10
    201,202c201,202
    <         - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.__PILLAR__DNS__DOMAIN__,5,SRV
    <         - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.__PILLAR__DNS__DOMAIN__,5,SRV
    ---
    >         - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,SRV
    >         - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,SRV


## 使用dockerhub获取最新gcr上的镜像教程


## 使用阿里云加速服务加速dockerhub上的镜像获取
    sudo mkdir -p /etc/docker
    sudo tee /etc/docker/daemon.json <<-'EOF'
    {
        "registry-mirrors": ["https://52fb9b02.mirror.aliyuncs.com"]
    }
    EOF
    sudo systemctl daemon-reload
    sudo systemctl restart docker

## 获取dockerhub镜像
    所有节点都需要pause容器
    sudo /kubernetes/local/bin/docker pull anjia0532/pause-amd64:3.1
    sudo /kubernetes/local/bin/docker images
    sudo /kubernetes/local/bin/docker tag anjia0532/pause-amd64:3.1 k8s.gcr.io/pause-amd64:3.1

    管理节点需要

## 看看DNS装没装好
    #安装
    kubectl create -f kube-dns.yaml

    kubectl get pods -l k8s-app=kube-dns -n kube-system
    应该输出如下
    NAME                        READY     STATUS    RESTARTS   AGE
    kube-dns-57b95f54f9-b9jwz   3/3       Running   0          27s

    如果READY是0/3，那么用下面命令查看错误信息
    kubectl describe pods -l k8s-app=kube-dns -n kube-system

    可以看到，绝大部分情况是因为k8s.gcr.io连不上，你猜为什么连不上？
    当然，也可能是因为dockerhub连不上，dockerhub托管在AWS上的，你猜为什么连不上？

    #把所有镜像手动拉到本地，然后重新安装,不重装的话会一直尝试拉取镜像用不了
    kubectl delete -f kube-dns.yaml
    kubectl create -f kube-dns.yaml

## 验证
    节选自kubenetes-the-hard-way，此方法的原理是创建一个busybox，3600秒后自动退出，使用kubectl exec在busybox里查看dns解析是否有问题，如果显示出来kubernetes的解析地址就是正确安装了dns服务
    #Create a busybox deployment:

    kubectl run busybox --image=busybox --command -- sleep 3600
    #List the pod created by the busybox deployment:

    kubectl get pods -l run=busybox
    #output

    NAME                       READY     STATUS    RESTARTS   AGE
    busybox-2125412808-mt2vb   1/1       Running   0          15s
    Retrieve the full name of the busybox pod:

    POD_NAME=$(kubectl get pods -l run=busybox -o jsonpath="{.items[0].metadata.name}")
    #Execute a DNS lookup for the kubernetes service inside the busybox pod:

    kubectl exec -ti $POD_NAME -- nslookup kubernetes
    #output

    Server:    10.32.0.10
    Address 1: 10.32.0.10 kube-dns.kube-system.svc.cluster.local

    Name:      kubernetes
    Address 1: 10.32.0.1 kubernetes.default.svc.cluster.local

    错误情况：
    nslookup: can't resolve 'kubernetes'
    command terminated with exit code 1
    此错误出现证明kubedns配置有问题，接下来去发现问题：
    #创建一个nginx pod：
    cat > pod-nginx.yaml<<EOF
    apiVersion: v1
    kind: Pod
    metadata:
      name: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
    EOF

    #进入pod查看resolv.conf
    kubectl exec nginx -i -t -- /bin/bash
    root@nginx:/# cat /etc/resolv.conf

    #output
    nameserver 192.254.0.2
    search default.svc.cluster.local svc.cluster.local cluster.local
    options ndots:5

    检查nameserver的ip地址是否正确；
    检查域名是否正确，如上述输出的域名是cluster.local，需要检查：
        #kubelet设置
        cat kubelet.service
        #kubelet的认证,sans里是否有kubernetes.default.svc.cluster.local
        cfssl-certinfo -cert kubernetes.pem
        #kube-dns的配置kube-dns.yaml要把所有__PILLAR__DNS__DOMAIN__都改成cluster.local
        cat kube-dns.yaml

# kube-dashboard

## 安装插件
    wget https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
    kubectl create -f kubernetes-dashboard.yaml
    #看看安上没有
    kubectl get pods -n kube-system
    #看看service
    kubectl get svc -n kube-system
    #看看日志，没啥大错就行，少个heapester插件是正常的
    kubectl logs kubernetes-dashboard-7c458d8866-gdrkr -n kube-system

## 访问
    kubectl -n kube-system edit service kubernetes-dashboard
    # Please edit the object below. Lines beginning with a '#' will be ignored,
    # and an empty file will abort the edit. If an error occurs while saving this file will be
    # reopened with the relevant failures.
    #
    apiVersion: v1
    ...
      name: kubernetes-dashboard
      namespace: kube-system
      resourceVersion: "343478"
      selfLink: /api/v1/namespaces/kube-system/services/kubernetes-dashboard-head
      uid: 8e48f478-993d-11e7-87e0-901b0e532516
    spec:
      clusterIP: 10.100.124.90
      externalTrafficPolicy: Cluster
      ports:
      - port: 443
        protocol: TCP
        targetPort: 8443
      selector:
        k8s-app: kubernetes-dashboard
      sessionAffinity: None
      type: ClusterIP
    status:
      loadBalancer: {}

    把这行的clusterIP改成NodePort按理说就能访问了

    kubectl get svc -n kube-system
    输出：
    NAME                   TYPE        CLUSTER-IP        EXTERNAL-IP   PORT(S)         AGE
    heapster               ClusterIP   192.254.138.19    <none>        80/TCP          55m
    kube-dns               ClusterIP   192.254.0.2       <none>        53/UDP,53/TCP   7h
    kubernetes-dashboard   NodePort    192.254.167.112   <none>        443:30759/TCP   42m
    monitoring-grafana     ClusterIP   192.254.248.204   <none>        80/TCP          57m
    monitoring-influxdb    ClusterIP   192.254.85.89     <none>        8086/TCP        57m

    查看dashboard那项，发现映射到了30759上了，使用https://master:port就能访问了，理论上是。

## 发现问题
    我发现这个dashboard的端口映射在了node的30759上，于是在其他节点使用IP:30759进行访问。在dashboard版本1.8以上时，已经不允许进行http访问，原文如下：
    NOTE: Dashboard should not be exposed publicly over HTTP. For domains accessed over HTTP it will not be possible to sign in. Nothing will happen after clicking Sign in button on login page.
    我在之前的设置中，已经禁止了匿名访问，也符合此项要求。于是使用https://IP:port进行访问，返回如下结果：
        您的连接不是私密连接
    并且这个页面的高级里没有继续访问的按钮。那么既然nodeport方法无法访问，APIserver能不能访问呢？执行如下命令：
    kubectl cluster-info
    只有如下两条记录
    Kubernetes master is running at https://192.168.12.165:6443
    KubeDNS is running at https://192.168.12.165:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

    也就是说并没有把dashborad发布成service，没办法通过apiserver的方式访问。

## 大致解决问题
    想到wget回来的yaml文件是dashboard的最简配置，于是便尝试使用kuberenetes源码中的addons里面的yaml进行创建。
    cd kubernetes/cluster/addons/dashboard/
    vim dashboard.service
    在最下面加上：
        type: NodePort
    kubectl create -f .
    kubectl get svc -n kube-system
    找到映射端口，用https访问，发现依旧是不是私密连接，emmmmm...
    kubectl cluster-info
    kubernetes-dashboard is running at https://192.168.12.165:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy

    发现已经被发布成为一个service了。

## APIserver访问方式
    前文提到了apiserver配置中已经禁止了匿名访问，所以直接访问service是无法登录的，所以我们需要生成一个证书导入到访问系统的浏览器里，
    openssl pkcs12 -export -in admin.pem -inkey admin-key.pem -out kube-admin.p12
    将kube-admin.p12导入到浏览器中即可：
    chrome下是设置->搜索ssl->管理证书，然后照着说明直接导入。

    访问https://192.168.12.165:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy
    选择证书之后发现依旧不是私密连接，但这次高级里可以继续访问了....
    登录方式如果照着kubernetes-the-hard-way或者opsnull教程搭建的可以选择token方式登录。

    创建一个admin-user.yaml文件。先搞一个admin-user的用户，然后把它绑在cluster-admin下。
    cat admin-user.yaml
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: admin-user
      namespace: kube-system
    ---
    apiVersion: rbac.authorization.k8s.io/v1beta1
    kind: ClusterRoleBinding
    metadata:
      name: admin-user
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: cluster-admin
    subjects:
    - kind: ServiceAccount
      name: admin-user
      namespace: kube-system

    kubectl create -f admin-user.yaml
    kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')

    应当输出如下信息：
    Name:         admin-user-token-6gl6l
    Namespace:    kube-system
    Labels:       <none>
    Annotations:  kubernetes.io/service-account.name=admin-user
                  kubernetes.io/service-account.uid=b16afba9-dfec-11e7-bbb9-901b0e532516

    Type:  kubernetes.io/service-account-token

    Data
    ====
    ca.crt:     1025 bytes
    namespace:  11 bytes
    token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLTZnbDZsIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJiMTZhZmJhOS1kZmVjLTExZTctYmJiOS05MDFiMGU1MzI1MTYiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.M70CU3lbu3PP4OjhFms8PVL5pQKj-jj4RNSLA4YmQfTXpPUuxqXjiTf094_Rzr0fgN_IVX6gC4fiNUL5ynx9KU-lkPfk0HnX8scxfJNzypL039mpGt0bbe1IXKSIRaq_9VW59Xz-yBUhycYcKPO9RM2Qa1Ax29nqNVko4vLn1_1wPqJ6XSq3GYI8anTzV8Fku4jasUwjrws6Cn6_sPEGmL54sq5R4Z5afUtv-mItTmqZZdxnkRqcJLlg2Y8WbCPogErbsaCDJoABQ7ppaqHetwfM_0yMun6ABOQbIwwl8pspJhpplKwyo700OSpvTT9zlBsu-b35lzXGBRHzv5g_RA

    把token复制下来登录即可。

## 遗留的问题
    NodePort访问还是不行，究竟应该导入什么证书才能让chrome认为它是安全的。


# heapster、grafana、influxdb
    这三个一起装。
    https://github.com/kubernetes/heapster/releases下载
    tar -zxvf heapster-1.5.3
    cd heapster-1.5.3/deploy/kube-config/influxdb

## 修改配置文件

    # heapster.yaml
    需要将heapster和system:heapster绑定，加入如下段
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: heapster
    subjects:
      - kind: ServiceAccount
        name: heapster
        namespace: kube-system
    roleRef:
      kind: ClusterRole
      name: system:heapster
      apiGroup: rbac.authorization.k8s.io
    将gcr.io/kubernetes链接改成anjia0532（注：一开始我通过dockerhub把gcr的image拉下来，后来发现有人干这事了而且一直在维护就直接用了，自己拉毕竟怪麻烦的，每个image都要创建一个dockerfile并push到github上，然后还得一顿操作，如果以后anjia0532没有对应版本，建议还是自己用dockerhub拉镜像，或者用某些科学的方法连接google）

    # influxdb.yaml
    image: anjia0532/heapster-influxdb-amd64:v1.3.3

    # grafana.yaml
    image: anjia0532/heapster-grafana-amd64:v4.4.3

## 启动
    kubectl create -f .

## 验证
    kubectl get svc -n kube-system
    # output

    NAME                   TYPE        CLUSTER-IP        EXTERNAL-IP   PORT(S)         AGE
    heapster               ClusterIP   192.254.138.19    <none>        80/TCP          1d
    kube-dns               ClusterIP   192.254.0.2       <none>        53/UDP,53/TCP   2d
    kubernetes-dashboard   NodePort    192.254.167.112   <none>        443:30759/TCP   1d
    monitoring-grafana     ClusterIP   192.254.248.204   <none>        80/TCP          1d
    monitoring-influxdb    ClusterIP   192.254.85.89     <none>        8086/TCP        1d

    kubectl get pods -n kube-system
    # output

    NAME                                    READY     STATUS    RESTARTS   AGE
    heapster-d59d66579-ptwj6                1/1       Running   0          1d
    kube-dns-57b95f54f9-b9jwz               3/3       Running   0          2d
    kubernetes-dashboard-5bc57d65cf-n8wrm   1/1       Running   0          1d
    monitoring-grafana-bff95c48c-w4k8z      1/1       Running   0          1d
    monitoring-influxdb-5d474bf6d5-dhspk    1/1       Running   0          1d

    kubectl logs monitoring-influxdb-5d474bf6d5-dhspk -n kube-system
    kubectl logs monitoring-grafana-bff95c48c-w4k8z -n kube-system
    kubectl logs heapster-d59d66579-ptwj6 -n kube-system

    都没啥错就行了，如果还想继续验证建议访问
    kubectl cluster-info
    https://192.168.12.165:6443/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy

# 解决网络访问不通的问题
    看node IP，ping一下和curl一下
    iptables -P FORWARD ACCEPT
