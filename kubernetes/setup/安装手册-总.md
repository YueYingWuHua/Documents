[TOC]

--------------------------
# kubernetes常用操作
## 部署时常用操作
```shell
  sudo systemctl stop xxx.service
  sudo cp xxx /etc/systemd/system/xxx.service
  systemctl daemon-reload
  sudo systemctl enable xxx.service
  sudo systemctl restart xxx.service
  sudo netstat -lnpt|grep kube-control
  journalctl -xefu kubelet
  journalctl -xe
  journalctl -u kube-apiserver
```

## 使用时常用操作
```shell
  kubectl get svc -n kube-system
  kubectl get svc --all-namespaces
  kubectl get nodes --all-namespaces
  kubectl create -f xxxx.yaml
  kubectl delete -f xxxx.yaml
  kubectl exec nginx -i -t -- /bin/bash
  kubectl cluster-info
  kubectl get pods --namespace=kube-system
  kubectl logs kubernetes-dashboard-5bc57d65cf-n8wrm -n kube-system
  kubectl logs coredns-7b9c599478-d9tj9 -n kube-system
  kubectl get endpoints kube-scheduler --namespace=kube-system  -o yaml
  kubectl get clusterrolebindings heapster -o yaml
  kubectl describe clusterrole heapster-custom-fix
  #获取token用来登录  
  kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}') kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')
```
# etcd
    生成key时要通过配置中的hosts或者在生成时指定--host，不然会导致证书不可用

    for ip in ${NODE_IPS}; do
    ETCDCTL_API=3 /kubernetes/local/bin/etcdctl \
    --endpoints=https://${ip}:2379 \
    --cacert=/etc/kubernetes/ssl/ca.pem \
    --cert=/etc/etcd/ssl/etcd.pem \
    --key=/etc/etcd/ssl/etcd-key.pem \
    endpoint health; done

# flannel
    ifconfig flannel.1
    ping 172.30.12.0
    ping 172.30.76.0
    ping 172.30.18.0

# kube-apiserver

已经没有token验证方式了，需要指定kubelet需要的验证文件
创建一个encryption-config.yaml，暂时不知道有啥用

kube-apiserver安装方式有多种，单节点：易于部署，但是容错差，一种replica，一种HA，对于云上安装可以不考虑HA直接采用google cloud的服务或者AWS的服务，如果物理机部署推荐采用replica方式。但是部署调试时建议使用单机模式。

api-server测试过程：

## api-server问题记录
1、journalctl -xe中如下日志刷一大堆，而且时时刻刻在刷；
```shell
    1月 07 14:40:16 cloud25 kube-apiserver[19798]: I0107 14:40:16.695016   19798 wrap.go:42] GET /apis/admissionregistration.k8s.io/v1alpha1/initializerconfigurations: (2.686028ms) 404 [[kube-apiserver/v1.10.2 (linux/amd64) kubernetes/81753b1
```
这个是dymanic admission control的问题,此项特性在目前的版本（1.10.2）下是测试特性，默认关闭需要手动开启，然而开启这个特性需要在启动的时候加入两个参数，如果并且需要指定下面两项（enable-admission-plugins中加入Initializers，并且--runtime-config加入admissionregistration.k8s.io/v1alpha1）：
```shell
 --enable-admission-plugins=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \
 --runtime-config=api/all,admissionregistration.k8s.io/v1alpha1 \
```

注：在多数手动安装教程中，--runtime-config=api/all \ 在实际安装中，可以先不加后面那项，如果遇到问题了再加上也不麻烦。

2、journalctl -xe中如下日志刷一大堆，而且时时刻刻在刷；
```shell
    1月 07 14:40:16 cloud25 kube-apiserver[19798]: I0107 14:40:16.695016   19798 wrap.go:42] GET /apis/admissionregistration.k8s.io/v1alpha1/initializerconfigurations: (2.686028ms) 200 [[kube-apiserver/v1.10.2 (linux/amd64) kubernetes/81753b1
```
看起来和上面一样，其实是404换成了200。这些日志应该没有太多影响，只要kubectl能正常获取系统配置就行。但是这2秒17条实在是太影响日志可读性了。journalctl -u kube-apiserver中的所有有用信息都被淹没在这些东西里了。

3、端口不对，apiserver指定的是6443端口。
看起来是一个小问题，然而这问题耽误了我半天时间。首先要说明的是，不论怎么安装，之前都要创建~/.kube/config文件，在创建这个文件时是通过kubectl进行写入的，而且往往这个文件配置的比较早，实际配置apiserver的时候大多又不会手动指定端口，如果配置.kube/config中的配置时指定的不是6443端口，那么就难受了。
系统会提示：
```shell
  kubectl cluster-info
  Kubernetes master is running at https://192.168.12.165:xxxx
  The connection to the server 192.168.12.165:xxxx was refused - did you specify the right host or port?
```
我看到这日志的第一反应是连接被拒绝，是不是证书过期？或者说启动出错？配合上面提到的第2个2秒17条的日志效果拔群。总以为是启动问题，其实“Kubernetes master is running at https://192.168.12.165:xxxx”这句话改成“connecting to kubernetes master: https://192.168.12.165:xxxx”可能比较容易想到是IP或者端口指定的问题。我是最后发现netstat -ano | grep 8443没启动，查stackoverflow时发现有人遇到类似的问题，最后通过修改./kube/config解决的，我才反应过来要查一下apiserver的默认端口，一查是6443然后发现已经监听好久了。

# docker
    需要在配置里指定PATH，不然找不到其他运行脚本

# kubelet
    1、记得把workspace路径提前创建好

## kubelet问题记录
    在开始运行kubelet时会进行flags的检测，检测一遍系统参数之后通过--config指定的config文件进行下一步的配置读取。那么问题来了：
    flags自检通过了之后，config的自检不显示在日志中，emmmmmmm....例如下面这个沙雕日志：
    journalctl -xefu kubelet

```shell
1月 09 17:15:49 cloud25 kubelet[2672]: Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.969989    2672 flags.go:27] FLAG: --address="0.0.0.0"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970212    2672 flags.go:27] FLAG: --allow-privileged="false"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970224    2672 flags.go:27] FLAG: --alsologtostderr="true"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970233    2672 flags.go:27] FLAG: --anonymous-auth="true"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970239    2672 flags.go:27] FLAG: --application-metrics-count-limit="100"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970245    2672 flags.go:27] FLAG: --authentication-token-webhook="false"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970251    2672 flags.go:27] FLAG: --authentication-token-webhook-cache-ttl="2m0s"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970261    2672 flags.go:27] FLAG: --authorization-mode="AlwaysAllow"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970269    2672 flags.go:27] FLAG: --authorization-webhook-cache-authorized-ttl="5m0s"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970276    2672 flags.go:27] FLAG: --authorization-webhook-cache-unauthorized-ttl="30s"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970281    2672 flags.go:27] FLAG: --azure-container-registry-config=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970287    2672 flags.go:27] FLAG: --boot-id-file="/proc/sys/kernel/random/boot_id"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970294    2672 flags.go:27] FLAG: --bootstrap-checkpoint-path=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970300    2672 flags.go:27] FLAG: --bootstrap-kubeconfig=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970306    2672 flags.go:27] FLAG: --cadvisor-port="4194"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970316    2672 flags.go:27] FLAG: --cert-dir="/var/lib/kubelet/pki"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970323    2672 flags.go:27] FLAG: --cgroup-driver="cgroupfs"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970329    2672 flags.go:27] FLAG: --cgroup-root=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970335    2672 flags.go:27] FLAG: --cgroups-per-qos="true"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970341    2672 flags.go:27] FLAG: --chaos-chance="0"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970351    2672 flags.go:27] FLAG: --client-ca-file=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970357    2672 flags.go:27] FLAG: --cloud-config=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970363    2672 flags.go:27] FLAG: --cloud-provider=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970368    2672 flags.go:27] FLAG: --cluster-dns="[]"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970383    2672 flags.go:27] FLAG: --cluster-domain=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970389    2672 flags.go:27] FLAG: --cni-bin-dir=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970394    2672 flags.go:27] FLAG: --cni-conf-dir=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970400    2672 flags.go:27] FLAG: --config="/kubernetes/config/kubelet.config.json"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970407    2672 flags.go:27] FLAG: --container-hints="/etc/cadvisor/container_hints.json"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970413    2672 flags.go:27] FLAG: --container-log-max-files="5"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970419    2672 flags.go:27] FLAG: --container-log-max-size="10Mi"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970425    2672 flags.go:27] FLAG: --container-runtime="docker"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970431    2672 flags.go:27] FLAG: --container-runtime-endpoint="unix:///var/run/dockershim.sock"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970437    2672 flags.go:27] FLAG: --containerd="unix:///var/run/containerd.sock"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970443    2672 flags.go:27] FLAG: --containerized="false"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970449    2672 flags.go:27] FLAG: --contention-profiling="false"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970454    2672 flags.go:27] FLAG: --cpu-cfs-quota="true"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970460    2672 flags.go:27] FLAG: --cpu-manager-policy="none"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970466    2672 flags.go:27] FLAG: --cpu-manager-reconcile-period="10s"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970471    2672 flags.go:27] FLAG: --docker="unix:///var/run/docker.sock"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970478    2672 flags.go:27] FLAG: --docker-disable-shared-pid="true"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970483    2672 flags.go:27] FLAG: --docker-endpoint="unix:///var/run/docker.sock"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970489    2672 flags.go:27] FLAG: --docker-env-metadata-whitelist=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970495    2672 flags.go:27] FLAG: --docker-only="false"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970500    2672 flags.go:27] FLAG: --docker-root="/var/lib/docker"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970506    2672 flags.go:27] FLAG: --docker-tls="false"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970512    2672 flags.go:27] FLAG: --docker-tls-ca="ca.pem"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970518    2672 flags.go:27] FLAG: --docker-tls-cert="cert.pem"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970524    2672 flags.go:27] FLAG: --docker-tls-key="key.pem"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970530    2672 flags.go:27] FLAG: --dynamic-config-dir=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970539    2672 flags.go:27] FLAG: --enable-controller-attach-detach="true"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970548    2672 flags.go:27] FLAG: --enable-custom-metrics="false"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970554    2672 flags.go:27] FLAG: --enable-debugging-handlers="true"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970560    2672 flags.go:27] FLAG: --enable-load-reader="false"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970566    2672 flags.go:27] FLAG: --enable-server="true"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970572    2672 flags.go:27] FLAG: --enforce-node-allocatable="[pods]"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970586    2672 flags.go:27] FLAG: --event-burst="10"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970592    2672 flags.go:27] FLAG: --event-qps="5"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970599    2672 flags.go:27] FLAG: --event-storage-age-limit="default=0"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970605    2672 flags.go:27] FLAG: --event-storage-event-limit="default=0"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970610    2672 flags.go:27] FLAG: --eviction-hard="imagefs.available<15%,memory.available<100Mi,nodefs.available<10%,nodefs.inodesFree<5%"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970629    2672 flags.go:27] FLAG: --eviction-max-pod-grace-period="0"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970636    2672 flags.go:27] FLAG: --eviction-minimum-reclaim=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970645    2672 flags.go:27] FLAG: --eviction-pressure-transition-period="5m0s"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970651    2672 flags.go:27] FLAG: --eviction-soft=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970657    2672 flags.go:27] FLAG: --eviction-soft-grace-period=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970664    2672 flags.go:27] FLAG: --exit-on-lock-contention="false"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970670    2672 flags.go:27] FLAG: --experimental-allocatable-ignore-eviction="false"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970676    2672 flags.go:27] FLAG: --experimental-allowed-unsafe-sysctls="[]"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970686    2672 flags.go:27] FLAG: --experimental-bootstrap-kubeconfig=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970693    2672 flags.go:27] FLAG: --experimental-check-node-capabilities-before-mount="false"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970699    2672 flags.go:27] FLAG: --experimental-dockershim="false"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970705    2672 flags.go:27] FLAG: --experimental-dockershim-root-directory="/var/lib/dockershim"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970711    2672 flags.go:27] FLAG: --experimental-fail-swap-on="false"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970716    2672 flags.go:27] FLAG: --experimental-kernel-memcg-notification="false"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970722    2672 flags.go:27] FLAG: --experimental-mounter-path=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970728    2672 flags.go:27] FLAG: --experimental-qos-reserved=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970734    2672 flags.go:27] FLAG: --fail-swap-on="false"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970740    2672 flags.go:27] FLAG: --feature-gates=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970748    2672 flags.go:27] FLAG: --file-check-frequency="20s"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970754    2672 flags.go:27] FLAG: --global-housekeeping-interval="1m0s"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970761    2672 flags.go:27] FLAG: --google-json-key=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970767    2672 flags.go:27] FLAG: --hairpin-mode="promiscuous-bridge"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970774    2672 flags.go:27] FLAG: --healthz-bind-address="127.0.0.1"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970780    2672 flags.go:27] FLAG: --healthz-port="10248"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970786    2672 flags.go:27] FLAG: --help="false"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970792    2672 flags.go:27] FLAG: --host-ipc-sources="[*]"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970805    2672 flags.go:27] FLAG: --host-network-sources="[*]"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970824    2672 flags.go:27] FLAG: --host-pid-sources="[*]"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970836    2672 flags.go:27] FLAG: --hostname-override=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970842    2672 flags.go:27] FLAG: --housekeeping-interval="10s"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970848    2672 flags.go:27] FLAG: --http-check-frequency="20s"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970854    2672 flags.go:27] FLAG: --image-gc-high-threshold="85"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970860    2672 flags.go:27] FLAG: --image-gc-low-threshold="80"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970866    2672 flags.go:27] FLAG: --image-pull-progress-deadline="2m0s"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970872    2672 flags.go:27] FLAG: --image-service-endpoint=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970877    2672 flags.go:27] FLAG: --iptables-drop-bit="15"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970883    2672 flags.go:27] FLAG: --iptables-masquerade-bit="14"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970889    2672 flags.go:27] FLAG: --keep-terminated-pod-volumes="false"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970895    2672 flags.go:27] FLAG: --kube-api-burst="10"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970900    2672 flags.go:27] FLAG: --kube-api-content-type="application/vnd.kubernetes.protobuf"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970909    2672 flags.go:27] FLAG: --kube-api-qps="5"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970915    2672 flags.go:27] FLAG: --kube-reserved=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970921    2672 flags.go:27] FLAG: --kube-reserved-cgroup=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970927    2672 flags.go:27] FLAG: --kubeconfig="/kubernetes/config/cloud25.kubeconfig"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970934    2672 flags.go:27] FLAG: --kubelet-cgroups=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970940    2672 flags.go:27] FLAG: --lock-file=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970946    2672 flags.go:27] FLAG: --log-backtrace-at=":0"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970954    2672 flags.go:27] FLAG: --log-cadvisor-usage="false"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970960    2672 flags.go:27] FLAG: --log-dir=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970966    2672 flags.go:27] FLAG: --log-flush-frequency="5s"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970973    2672 flags.go:27] FLAG: --logtostderr="false"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970979    2672 flags.go:27] FLAG: --machine-id-file="/etc/machine-id,/var/lib/dbus/machine-id"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970986    2672 flags.go:27] FLAG: --make-iptables-util-chains="true"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970992    2672 flags.go:27] FLAG: --manifest-url=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970998    2672 flags.go:27] FLAG: --manifest-url-header=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.971008    2672 flags.go:27] FLAG: --master-service-namespace="default"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.971014    2672 flags.go:27] FLAG: --max-open-files="1000000"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.971023    2672 flags.go:27] FLAG: --max-pods="110"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.971029    2672 flags.go:27] FLAG: --maximum-dead-containers="-1"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.971035    2672 flags.go:27] FLAG: --maximum-dead-containers-per-container="1"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.971041    2672 flags.go:27] FLAG: --minimum-container-ttl-duration="0s"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.971047    2672 flags.go:27] FLAG: --minimum-image-ttl-duration="2m0s"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.971053    2672 flags.go:27] FLAG: --network-plugin=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.971059    2672 flags.go:27] FLAG: --network-plugin-mtu="0"
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.971065    2672 flags.go:27] FLAG: --node-ip=""
1月 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.971071    2672 flags.go:27] FLAG: --node-labels=""
1月 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971079    2672 flags.go:27] FLAG: --node-status-update-frequency="10s"
1月 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971085    2672 flags.go:27] FLAG: --non-masquerade-cidr="10.0.0.0/8"
1月 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971091    2672 flags.go:27] FLAG: --oom-score-adj="-999"
1月 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971098    2672 flags.go:27] FLAG: --pod-cidr=""
1月 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971103    2672 flags.go:27] FLAG: --pod-infra-container-image="anjia0532/pause-amd64:3.1"
1月 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971110    2672 flags.go:27] FLAG: --pod-manifest-path=""
1月 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971115    2672 flags.go:27] FLAG: --pod-max-pids="-1"
1月 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971121    2672 flags.go:27] FLAG: --pods-per-core="0"
1月 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971127    2672 flags.go:27] FLAG: --port="10250"
1月 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971133    2672 flags.go:27] FLAG: --protect-kernel-defaults="false"
1月 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971139    2672 flags.go:27] FLAG: --provider-id=""
1月 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971145    2672 flags.go:27] FLAG: --read-only-port="10255"
1月 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971151    2672 flags.go:27] FLAG: --really-crash-for-testing="false"
1月 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971156    2672 flags.go:27] FLAG: --register-node="true"
1月 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971162    2672 flags.go:27] FLAG: --register-schedulable="true"
1月 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971169    2672 flags.go:27] FLAG: --register-with-taints=""
1月 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971177    2672 flags.go:27] FLAG: --registry-burst="10"
1月 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971183    2672 flags.go:27] FLAG: --registry-qps="5"
1月 09 17:15:50 cloud25 systemd[1]: kubelet.service: Main process exited, code=exited, status=255/n/a
1月 09 17:15:50 cloud25 systemd[1]: kubelet.service: Unit entered failed state.
1月 09 17:15:50 cloud25 systemd[1]: kubelet.service: Failed with result 'exit-code'.
1月 09 17:15:55 cloud25 systemd[1]: kubelet.service: Service hold-off time over, scheduling restart.
1月 09 17:15:55 cloud25 systemd[1]: Stopped Kubernetes Kubelet.
-- Subject: Unit kubelet.service has finished shutting down
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
--
-- Unit kubelet.service has finished shutting down.
1月 09 17:15:55 cloud25 systemd[1]: Started Kubernetes Kubelet.
-- Subject: Unit kubelet.service has finished start-up
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
--
-- Unit kubelet.service has finished starting up.
--
-- The start-up result is done.
1月 09 17:15:56 cloud25 kubelet[2762]: W0109 17:15:55.813586    2762 docker_sandbox.go:353] failed to read pod IP from plugin/docker: Couldn't find network status for kube-system/kube-dns-57b95f54f9-b9jwz through plugin: invalid network status for
1月 09 17:15:56 cloud25 systemd[1]: kubelet.service: Main process exited, code=exited, status=255/n/a
1月 09 17:15:56 cloud25 systemd[1]: kubelet.service: Unit entered failed state.
1月 09 17:15:56 cloud25 systemd[1]: kubelet.service: Failed with result 'exit-code'.
1月 09 17:15:56 cloud25 systemd[1]: Stopped Kubernetes Kubelet.
-- Subject: Unit kubelet.service has finished shutting down
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
--
-- Unit kubelet.service has finished shutting down.
1月 09 17:28:20 cloud25 systemd[1]: Stopped Kubernetes Kubelet.
-- Subject: Unit kubelet.service has finished shutting down
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
--
-- Unit kubelet.service has finished shutting down.
1月 09 17:28:30 cloud25 systemd[1]: Stopped Kubernetes Kubelet.
-- Subject: Unit kubelet.service has finished shutting down
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
--
-- Unit kubelet.service has finished shutting down.
```

我在想这个日志出的问题唯一一个问题是kube-dns组件找不到，但是不应该启动不起来。于是去google查了一下，在github上有类似的issue，但是两个问题的解决方案分别是：关闭swap，我这里添加了flag: --fail-swap-on=false，应该不是这个问题。还有一个是找不到ca-pem，但是我config文件指定了。
试了好久发现是config文件里的ca-key.pem对应的key写错了。读配置时应当是没读出来。所以部署kubelet时一定要注意查看自己的config文件是否书写正确，尤其是不论json还是yaml都不大好写。

2、下载不下来pause容器
由于pause容器是kubernetes的基础容器，所以部署Kubelet时会下载这个容器，但是由于某些原因，这个网站并不能访问。所以需要在kubelet.service中加一项设置:
```shell
 --pod-infra-container-image=anjia0532/pause-amd64:3.1 \
```
此镜像是国人写的自动从某个不能访问的网站拉取的，更新及时，比自己通过dockerhub转拉方便的多，至少省的设置dockerhub外部链接那些步骤。

# kube-proxy

# 功能测试
## 遇到的问题
1、当创建完几个测试用pod之后，使用kubectl get pods 返回No resources found.
当create -f 返回created之后，创建命令被分发到controller manager进行处理，此时如果controller manager出现什么问题，那么就会导致创建Pod失败。
在本次测试中，出现的问题是连接不上kube-apiserver，检查kube-controller-manager.kubeconfig中的server是否正确，这个例子中port应该是6443而不是8443：
```shell
journalctl -xefu kube-controller-manager
1月 10 12:54:03 cloud25 kube-controller-manager[12390]: E0110 12:54:03.903735   12390 leaderelection.go:242] error retrieving resource lock kube-system/kube-controller-manager: Get https://192.168.12.165:8443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 192.168.12.165:8443: getsockopt: connection refused
1月 10 12:54:05 cloud25 kube-controller-manager[12390]: E0110 12:54:05.972148   12390 leaderelection.go:242] error retrieving resource lock kube-system/kube-controller-manager: Get https://192.168.12.165:8443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 192.168.12.165:8443: getsockopt: connection refused
1月 10 12:54:07 cloud25 kube-controller-manager[12390]: E0110 12:54:07.980693   12390 leaderelection.go:242] error retrieving resource lock kube-system/kube-controller-manager: Get https://192.168.12.165:8443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 192.168.12.165:8443: getsockopt: connection refused
1月 10 12:54:11 cloud25 kube-controller-manager[12390]: E0110 12:54:11.739869   12390 leaderelection.go:242] error retrieving resource lock kube-system/kube-controller-manager: Get https://192.168.12.165:8443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 192.168.12.165:8443: getsockopt: connection refused
1月 10 12:54:15 cloud25 kube-controller-manager[12390]: E0110 12:54:15.174241   12390 leaderelection.go:242] error retrieving resource lock kube-system/kube-controller-manager: Get https://192.168.12.165:8443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 192.168.12.165:8443: getsockopt: connection refused
```

# kubeDNS（deprecated,目前使用coreDNS）
## 修改kube-dns.yaml
    #先去github摸个完整源码回来
    git clone https://github.com/kubernetes/kubernetes.git
    #cd kubernetes/cluster/addons/dns
    mv kube-dns.yaml.base kube-dns.yaml
    vim kube-dns.yaml
    #修改一些乱七八糟东西
    clusterIP是准备好的DNS地址，image反正连不上，爱改成哪都行，反正原理就是拉个镜像回来，yueyingwuhua这个是我自己在dockerhub上摸的镜像。
    cloud@cloud25:~/k8s$ diff kube-dns.yaml.base kube-dns.yaml
    33c33
    <   clusterIP: __PILLAR__DNS__SERVER__
    ---
    >   clusterIP: 192.254.0.2
    98c98
    <         image: k8s.gcr.io/k8s-dns-kube-dns-amd64:1.14.10
    ---
    >         image: yueyingwuhua/k8s-dns-kube-dns-amd64:1.14.10
    128c128
    <         - --domain=__PILLAR__DNS__DOMAIN__.
    ---
    >         - --domain=cluster.local.
    149c149
    <         image: k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.14.10
    ---
    >         image: yueyingwuhua/k8s-dns-dnsmasq-nanny-amd64:1.14.10
    169c169
    <         - --server=/__PILLAR__DNS__DOMAIN__/127.0.0.1#10053
    ---
    >         - --server=/cluster.local/127.0.0.1#10053
    188c188
    <         image: k8s.gcr.io/k8s-dns-sidecar-amd64:1.14.10
    ---
    >         image: yueyingwuhua/k8s-dns-sidecar-amd64:1.14.10
    201,202c201,202
    <         - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.__PILLAR__DNS__DOMAIN__,5,SRV
    <         - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.__PILLAR__DNS__DOMAIN__,5,SRV
    ---
    >         - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,SRV
    >         - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,SRV


## 使用dockerhub获取最新gcr上的镜像教程

  本部分内容旨在介绍如何通过dockerhub转储gcr.io上面的镜像，当需要使用这些镜像时可以通过dockerhub直接使用而不是通过目田上网工具。这个方法的原理是dockerhub支持外部镜像的构建，我们可以通过写dockerfile的方式将gcr.io上面的镜像直接原封不动的拖到dockerhub上，实现转储。  

### 在github上创建一个repo
  ![image text](https://raw.githubusercontent.com/YueYingWuHua/Documents/master/images/download_gcr_image/repo-for-dockerhub.jpg)
  如上图是我创建好的一个repo，里面已经有了4个文件，这4个文件对应的镜像都是在kubernetes setup过程中需要或曾经需要用到的。其中coredns的内容为：
  ```shell
  FROM k8s.gcr.io/coredns:1.0.6
  ```
  说明：dockerfile以外部镜像`k8s.gcr.io/coredns:1.0.6`作为基础镜像，之后什么都不做，那么这个dockerfile生成出来的镜像就和原始镜像完全相同。

### 登录dockerhub进行自动构建
  登录dockerhub，点击repositories,在页面中点击Create Repository:

  ![image text](https://raw.githubusercontent.com/YueYingWuHua/Documents/master/images/download_gcr_image/create_repository.jpg)

  输入repo名称和描述，其他不用变，直接点击create:

  ![image text](https://raw.githubusercontent.com/YueYingWuHua/Documents/master/images/download_gcr_image/create.jpg)

  之后需要进行一些自动化部署的配置，选择你创建好的那个空repo，在builds下面点击Configure Automated Builds：

  ![image text](https://raw.githubusercontent.com/YueYingWuHua/Documents/master/images/download_gcr_image/configure_build.jpg)

  如果之前没有连接过github，这时需要连接一下github，让github授权dockerhub可以读取文件即可。
  现在最上方选择dockerfile所在的repo，然后在底下build rules里面填写docker tag和dockerfile location，点击save and build。

  ![image text](https://raw.githubusercontent.com/YueYingWuHua/Documents/master/images/download_gcr_image/build_setting.jpg)

  等几分钟，刷新一下应该就可用了。接下来就是传统的：
  ```
  sudo docker pull yueyingwuhua/coredns:1.0.6
  sudo docker tag yueyingwuhua/coredns:1.0.6 k8s.gcr.io/coredns:1.0.6
  如果不想改tag或者以后还需要升级插件，建议直接修改插件yaml内容
  vim coredns.yaml
  cloud@cloud25:/kubernetes/componentyaml$ diff coredns.yaml ~/kubernetes/kubernetes-1.10.12/cluster/addons/dns/coredns.yaml.base
  61c61
  <         kubernetes guinai.cloudk8s in-addr.arpa ip6.arpa {
  ---
  >         kubernetes __PILLAR__DNS__DOMAIN__ in-addr.arpa ip6.arpa {
  103c103
  <         image: yueyingwuhua/coredns:1.0.6
  ---
  >         image: k8s.gcr.io/coredns:1.0.6
  153c153
  <   clusterIP: 192.254.0.2
  ---
  >   clusterIP: __PILLAR__DNS__SERVER__
  ```

## 使用阿里云加速服务加速dockerhub上的镜像获取
    sudo mkdir -p /etc/docker
    sudo tee /etc/docker/daemon.json <<-'EOF'
    {
        "registry-mirrors": ["https://52fb9b02.mirror.aliyuncs.com"]
    }
    EOF
    sudo systemctl daemon-reload
    sudo systemctl restart docker

## 获取dockerhub镜像
    所有节点都需要pause容器
    sudo /kubernetes/local/bin/docker pull anjia0532/pause-amd64:3.1
    sudo /kubernetes/local/bin/docker images
    sudo /kubernetes/local/bin/docker tag anjia0532/pause-amd64:3.1 k8s.gcr.io/pause-amd64:3.1


## 看看DNS装没装好
    #安装
    kubectl create -f kube-dns.yaml

    kubectl get pods -l k8s-app=kube-dns -n kube-system
    应该输出如下
    NAME                        READY     STATUS    RESTARTS   AGE
    kube-dns-57b95f54f9-b9jwz   3/3       Running   0          27s

    如果READY是0/3，那么用下面命令查看错误信息
    kubectl describe pods -l k8s-app=kube-dns -n kube-system

    可以看到，绝大部分情况是因为k8s.gcr.io连不上。（至于为什么连不上，emmmm....）
    当然，也可能是因为dockerhub连不上，dockerhub托管在AWS上的（至于为什么连不上，emm....）

    #把所有镜像手动拉到本地，然后重新安装,不重装的话会一直尝试拉取镜像用不了
    kubectl delete -f kube-dns.yaml
    kubectl create -f kube-dns.yaml

## 验证
    节选自kubenetes-the-hard-way，此方法的原理是创建一个busybox，3600秒后自动退出，使用kubectl exec在busybox里查看dns解析是否有问题，如果显示出来kubernetes的解析地址就是正确安装了dns服务
    #Create a busybox deployment:

    kubectl run busybox --image=busybox --command -- sleep 3600
    #List the pod created by the busybox deployment:

    kubectl get pods -l run=busybox
    #output

    NAME                       READY     STATUS    RESTARTS   AGE
    busybox-2125412808-mt2vb   1/1       Running   0          15s
    Retrieve the full name of the busybox pod:

    POD_NAME=$(kubectl get pods -l run=busybox -o jsonpath="{.items[0].metadata.name}")
    #Execute a DNS lookup for the kubernetes service inside the busybox pod:

    kubectl exec -ti $POD_NAME -- nslookup kubernetes
    #output

    Server:    10.32.0.10
    Address 1: 10.32.0.10 kube-dns.kube-system.svc.cluster.local

    Name:      kubernetes
    Address 1: 10.32.0.1 kubernetes.default.svc.cluster.local

    错误情况：
    nslookup: can't resolve 'kubernetes'
    command terminated with exit code 1
    此错误出现证明kubedns配置有问题，接下来去发现问题：
    #创建一个nginx pod：
    cat > pod-nginx.yaml<<EOF
    apiVersion: v1
    kind: Pod
    metadata:
      name: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
    EOF

    #进入pod查看resolv.conf
    kubectl exec nginx -i -t -- /bin/bash
    root@nginx:/# cat /etc/resolv.conf

    #output
    nameserver 192.254.0.2
    search default.svc.cluster.local svc.cluster.local cluster.local
    options ndots:5

    检查nameserver的ip地址是否正确；
    检查域名是否正确，如上述输出的域名是cluster.local，需要检查：
        #kubelet设置
        cat kubelet.service
        #kubelet的认证,sans里是否有kubernetes.default.svc.cluster.local
        cfssl-certinfo -cert kubernetes.pem
        #kube-dns的配置kube-dns.yaml要把所有__PILLAR__DNS__DOMAIN__都改成cluster.local
        cat kube-dns.yaml

# coreDNS

## 安装
## 测试
## 发现问题
## 解决方案
 reference：https://github.com/kubernetes/kubernetes/issues/66924
 经过一轮排查发现是ImagePullBackOff,你墙你🐴呢？


# kube-dashboard

## 安装插件
    wget https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
    kubectl create -f kubernetes-dashboard.yaml
    #看看安上没有
    kubectl get pods -n kube-system
    #看看service
    kubectl get svc -n kube-system
    #看看日志，没啥大错就行，少个heapester插件是正常的
    kubectl logs kubernetes-dashboard-7c458d8866-gdrkr -n kube-system

## 访问
    kubectl -n kube-system edit service kubernetes-dashboard
    # Please edit the object below. Lines beginning with a '#' will be ignored,
    # and an empty file will abort the edit. If an error occurs while saving this file will be
    # reopened with the relevant failures.
    #
    apiVersion: v1
    ...
      name: kubernetes-dashboard
      namespace: kube-system
      resourceVersion: "343478"
      selfLink: /api/v1/namespaces/kube-system/services/kubernetes-dashboard-head
      uid: 8e48f478-993d-11e7-87e0-901b0e532516
    spec:
      clusterIP: 10.100.124.90
      externalTrafficPolicy: Cluster
      ports:
      - port: 443
        protocol: TCP
        targetPort: 8443
      selector:
        k8s-app: kubernetes-dashboard
      sessionAffinity: None
      type: ClusterIP
    status:
      loadBalancer: {}

    把这行的clusterIP改成NodePort按理说就能访问了

    kubectl get svc -n kube-system
    输出：
    NAME                   TYPE        CLUSTER-IP        EXTERNAL-IP   PORT(S)         AGE
    heapster               ClusterIP   192.254.138.19    <none>        80/TCP          55m
    kube-dns               ClusterIP   192.254.0.2       <none>        53/UDP,53/TCP   7h
    kubernetes-dashboard   NodePort    192.254.167.112   <none>        443:30759/TCP   42m
    monitoring-grafana     ClusterIP   192.254.248.204   <none>        80/TCP          57m
    monitoring-influxdb    ClusterIP   192.254.85.89     <none>        8086/TCP        57m

    查看dashboard那项，发现映射到了30759上了，使用https://master:port就能访问了，理论上是。

## 发现问题
    我发现这个dashboard的端口映射在了node的30759上，于是在其他节点使用IP:30759进行访问。在dashboard版本1.8以上时，已经不允许进行http访问，原文如下：
    NOTE: Dashboard should not be exposed publicly over HTTP. For domains accessed over HTTP it will not be possible to sign in. Nothing will happen after clicking Sign in button on login page.
    我在之前的设置中，已经禁止了匿名访问，也符合此项要求。于是使用https://IP:port进行访问，返回如下结果：
        您的连接不是私密连接
    并且这个页面的高级里没有继续访问的按钮。那么既然nodeport方法无法访问，APIserver能不能访问呢？执行如下命令：
    kubectl cluster-info
    只有如下两条记录
    Kubernetes master is running at https://192.168.12.165:6443
    KubeDNS is running at https://192.168.12.165:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

    也就是说并没有把dashborad发布成service，没办法通过apiserver的方式访问。

## 大致解决问题
    想到wget回来的yaml文件是dashboard的最简配置，于是便尝试使用kuberenetes源码中的addons里面的yaml进行创建。
    cd kubernetes/cluster/addons/dashboard/
    vim dashboard.service
    在最下面加上：
        type: NodePort
    kubectl create -f .
    kubectl get svc -n kube-system
    找到映射端口，用https访问，发现依旧是不是私密连接，emmmmm...
    kubectl cluster-info
    kubernetes-dashboard is running at https://192.168.12.165:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy

    发现已经被发布成为一个service了。

## APIserver访问方式
    前文提到了apiserver配置中已经禁止了匿名访问，所以直接访问service是无法登录的，所以我们需要生成一个证书导入到访问系统的浏览器里，
    openssl pkcs12 -export -in admin.pem -inkey admin-key.pem -out kube-admin.p12
    将kube-admin.p12导入到浏览器中即可：
    chrome下是设置->搜索ssl->管理证书，然后照着说明直接导入。

    访问https://192.168.12.165:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy
    选择证书之后发现依旧不是私密连接，但这次高级里可以继续访问了....
    登录方式如果照着kubernetes-the-hard-way或者opsnull教程搭建的可以选择token方式登录。

    创建一个admin-user.yaml文件。先搞一个admin-user的用户，然后把它绑在cluster-admin下。
    cat admin-user.yaml
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: admin-user
      namespace: kube-system
    ---
    apiVersion: rbac.authorization.k8s.io/v1beta1
    kind: ClusterRoleBinding
    metadata:
      name: admin-user
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: cluster-admin
    subjects:
    - kind: ServiceAccount
      name: admin-user
      namespace: kube-system

    kubectl create -f admin-user.yaml
    kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')

    应当输出如下信息：
    Name:         admin-user-token-6gl6l
    Namespace:    kube-system
    Labels:       <none>
    Annotations:  kubernetes.io/service-account.name=admin-user
                  kubernetes.io/service-account.uid=b16afba9-dfec-11e7-bbb9-901b0e532516

    Type:  kubernetes.io/service-account-token

    Data
    ====
    ca.crt:     1025 bytes
    namespace:  11 bytes
    token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLTZnbDZsIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJiMTZhZmJhOS1kZmVjLTExZTctYmJiOS05MDFiMGU1MzI1MTYiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.M70CU3lbu3PP4OjhFms8PVL5pQKj-jj4RNSLA4YmQfTXpPUuxqXjiTf094_Rzr0fgN_IVX6gC4fiNUL5ynx9KU-lkPfk0HnX8scxfJNzypL039mpGt0bbe1IXKSIRaq_9VW59Xz-yBUhycYcKPO9RM2Qa1Ax29nqNVko4vLn1_1wPqJ6XSq3GYI8anTzV8Fku4jasUwjrws6Cn6_sPEGmL54sq5R4Z5afUtv-mItTmqZZdxnkRqcJLlg2Y8WbCPogErbsaCDJoABQ7ppaqHetwfM_0yMun6ABOQbIwwl8pspJhpplKwyo700OSpvTT9zlBsu-b35lzXGBRHzv5g_RA

    把token复制下来登录即可。

## 遗留的问题
    NodePort访问还是不行，究竟应该导入什么证书才能让chrome认为它是安全的。


# heapster、grafana、influxdb
    这三个一起装。
    https://github.com/kubernetes/heapster/releases下载
    tar -zxvf heapster-1.5.3
    cd heapster-1.5.3/deploy/kube-config/influxdb

## 修改配置文件

    # heapster.yaml
    需要将heapster和system:heapster绑定，加入如下段
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: heapster
    subjects:
      - kind: ServiceAccount
        name: heapster
        namespace: kube-system
    roleRef:
      kind: ClusterRole
      name: system:heapster
      apiGroup: rbac.authorization.k8s.io
    将gcr.io/kubernetes链接改成anjia0532（注：一开始我通过dockerhub把gcr的image拉下来，后来发现有人干这事了而且一直在维护就直接用了，自己拉毕竟怪麻烦的，每个image都要创建一个dockerfile并push到github上，然后还得一顿操作，如果以后anjia0532没有对应版本，建议还是自己用dockerhub拉镜像，或者用某些科学的方法连接google）
    注：现在全自己拉取了，故全部改为yueyingwuhua了

    # influxdb.yaml
    image: yueyingwuhua/heapster-influxdb-amd64:v1.3.3

    # grafana.yaml
    image: yueyingwuhua/heapster-grafana-amd64:v4.4.3

## 启动
    kubectl create -f .

## 验证
    kubectl get svc -n kube-system
    # output

    NAME                   TYPE        CLUSTER-IP        EXTERNAL-IP   PORT(S)         AGE
    heapster               ClusterIP   192.254.138.19    <none>        80/TCP          1d
    kube-dns               ClusterIP   192.254.0.2       <none>        53/UDP,53/TCP   2d
    kubernetes-dashboard   NodePort    192.254.167.112   <none>        443:30759/TCP   1d
    monitoring-grafana     ClusterIP   192.254.248.204   <none>        80/TCP          1d
    monitoring-influxdb    ClusterIP   192.254.85.89     <none>        8086/TCP        1d

    kubectl get pods -n kube-system
    # output

    NAME                                    READY     STATUS    RESTARTS   AGE
    heapster-d59d66579-ptwj6                1/1       Running   0          1d
    kube-dns-57b95f54f9-b9jwz               3/3       Running   0          2d
    kubernetes-dashboard-5bc57d65cf-n8wrm   1/1       Running   0          1d
    monitoring-grafana-bff95c48c-w4k8z      1/1       Running   0          1d
    monitoring-influxdb-5d474bf6d5-dhspk    1/1       Running   0          1d

    kubectl logs monitoring-influxdb-5d474bf6d5-dhspk -n kube-system
    kubectl logs monitoring-grafana-bff95c48c-w4k8z -n kube-system
    kubectl logs heapster-d59d66579-ptwj6 -n kube-system

    都没啥错就行了，如果还想继续验证建议访问
    kubectl cluster-info
    https://192.168.12.165:6443/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy

## 遇到的问题
### 1.10.12中heapster获取状态信息被拒绝
此问题的外在表现是grafana获取不到数据，显示全部为空，dashboard没有任何资源监控相关的内容。
```shell
> kubectl get pods -n kube-system
> kubectl logs heapster-xxxxxx-xxxxxxxx -n kube-system
 发现如下问题：

 E0116 02:34:09.090864       1 reflector.go:190] k8s.io/heapster/metrics/heapster.go:328: Failed to list *v1.Pod: pods is forbidden: User "system:serviceaccount:kube-system:heapster" cannot list pods at the cluster scope
```
从日志我们可以获取到的信息是，我们的serviceaccount：kube-system:heapster没有足够的权限获取pods信息，那么可以确定的是角色授权出了问题。
```shell
> kubectl get clusterrolebinding
发现有如下项：
heapster                                               3h
（如果没有请执行如下命令:
> cd ~/kubernetes/kubernetes-1.10.12/cluster/addons/heapster-1.5.4/deploy/kube-config/influxdb
> cd ../rbac
> kubectl create -f heapster-rbac.yaml）

看到我们确实有一个role与我们的serviceaccount，即heapster进行绑定了，接下来我们需要查看与之绑定的role权限是否正确。
> kubectl get clusterrolebindings heapster -o yaml
显示如下（这次是经过修改后的授权，如果是错误的请查看roleRef下的name项是否为对应的角色）：
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  creationTimestamp: 2019-01-16T02:56:02Z
  name: heapster
  resourceVersion: "1121099"
  selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/heapster
  uid: 4272aa37-193a-11e9-9f2c-6c92bf20cf87
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:heapster
subjects:
- kind: ServiceAccount
  name: heapster
  namespace: kube-system
> kubectl describe clusterrole system:heapster
Name:         system:heapster
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources               Non-Resource URLs  Resource Names  Verbs
  ---------               -----------------  --------------  -----
  events                  []                 []              [get list watch]
  namespaces              []                 []              [get list watch]
  nodes                   []                 []              [get list watch]
  pods                    []                 []              [get list watch]
  deployments.extensions  []                 []              [get list watch]
```
这时，我们确认了heapster对应的角色，如果不对，执行如下命令进行重新设置：
```shell
> cd ~/kubernetes/kubernetes-1.10.12/cluster/addons/heapster-1.5.4/deploy/kube-config/influxdb
> cd ../rbac
> kubectl delete -f heapster-rbac.yaml
> vim heapster-rbac.yaml
> kubectl create -f heapster-rbac.yaml
```
这种设置方式在kubernetes1.10.10之前不会出现其他问题，如果需要在新版本部署heapster，可能遇到下面所写明的错误。

接下来是新版本遇到的问题：
由于在新版本(1.10.10+)中，heapster+grafana+influxdb不再作为推荐的集群监控插件了 [^1]，此功能由Metric-server代替，并且heapster也被标记为retired并不再进行维护，新版本在部署heapster时可能会遇到越来越多的问题，在当前版本（1.13.2, 1.12.4, 1.11.6 以及本文涉及的1.10.12）中，部署时会遇到如下问题：
```shell
> kubectl logs heapster-xxxxxx-xxxxxxxx -n kube-system
E0116 02:42:05.040909       1 manager.go:101] Error in scraping containers from kubelet:192.168.12.165:10250: failed to get all container stats from Kubelet URL "https://192.168.12.165:10250/stats/container/": request failed - "403 Forbidden", response: "Forbidden (user=system:serviceaccount:kube-system:heapster, verb=create, resource=nodes, subresource=stats)"
```
此日志主要说明了为什么heapster失效，原因是kube-system:heapster的serviceaccount对于资源：nodes的子资源stats不支持create操作，暂且不论新版本对于权限管理进行了多少细化操作，我们要做的其实就是想办法将这个资源的create操作赋予我们的account即可。
回到上文，之前的命令中，我们执行`kubectl describe clusterrole system:heapster`已经获取了heapster所有权限了，确实没有对应的操作，这就必须手动创建一个角色，并把角色和serviceaccount绑定，来获取足够多的权限。首先创建一个包含heapster所有权限的role：
```yaml
> vim heapster-custom-fix.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: heapster-custom-fix
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - nodes
  - namespaces
  - events
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - extensions
  resources:
  - deployments
  verbs:
  - get
  - list
  - update
  - watch
- apiGroups:
  - ""
  resources:
  - nodes/stats
  verbs:
  - get
  - create

> kubectl create -f heapster-custom-fix.yaml
> vim heapster-rbac.yaml
把system:heapster修改为自定义角色heapster-custom-fix
最终文件为：
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: heapster
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: heapster-custom-fix
subjects:
- kind: ServiceAccount
  name: heapster
  namespace: kube-system

> kubectl create -f heapster-rbac.yaml
```
修改完之后记得delete -f + create -f heapster.yaml，重装一下heapster插件使我们的插件生效。

## grafana不能连接本地IP
外在表现：进grafana页面后找不到dashboard，提示需要手动创建。
使用`kubectl logs podname -n kube-system`获取日志：
```shell
Can't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp 127.0.0.1:3000: getsockopt: connection refused. Retrying after 5 seconds...
```
发现连接不上，此问题没有什么彻底的解决方案，在本次解决中，grafana.yaml中把type:NodePort重新注释掉就好了，相当于只修改了container的地址，其他都用的默认设置。

[^1]: references:
https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/
https://github.com/Azure/aks-engine/issues/73
https://github.com/kubernetes-retired/heapster/issues/1936

# 解决网络访问不通的问题
    看node IP，ping一下和curl一下
    iptables -P FORWARD ACCEPT
