[TOC]

--------------------------
# kuberneteså¸¸ç”¨æ“ä½œ
## éƒ¨ç½²æ—¶å¸¸ç”¨æ“ä½œ
```shell
  sudo systemctl stop xxx.service
  sudo cp xxx /etc/systemd/system/xxx.service
  systemctl daemon-reload
  sudo systemctl enable xxx.service
  sudo systemctl restart xxx.service
  sudo netstat -lnpt|grep kube-control
  journalctl -xefu kubelet
  journalctl -xe
  journalctl -u kube-apiserver
```

## ä½¿ç”¨æ—¶å¸¸ç”¨æ“ä½œ
```shell
  kubectl get svc -n kube-system
  kubectl get svc --all-namespaces
  kubectl get nodes --all-namespaces
  kubectl create -f xxxx.yaml
  kubectl delete -f xxxx.yaml
  kubectl exec nginx -i -t -- /bin/bash
  kubectl cluster-info
  kubectl get pods --namespace=kube-system
  kubectl logs kubernetes-dashboard-5bc57d65cf-n8wrm -n kube-system
  kubectl logs coredns-7b9c599478-d9tj9 -n kube-system
  kubectl get endpoints kube-scheduler --namespace=kube-system  -o yaml
  kubectl get clusterrolebindings heapster -o yaml
  kubectl describe clusterrole heapster-custom-fix
  #è·å–tokenç”¨æ¥ç™»å½•  
  kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}') kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')
```
# etcd
    ç”Ÿæˆkeyæ—¶è¦é€šè¿‡é…ç½®ä¸­çš„hostsæˆ–è€…åœ¨ç”Ÿæˆæ—¶æŒ‡å®š--hostï¼Œä¸ç„¶ä¼šå¯¼è‡´è¯ä¹¦ä¸å¯ç”¨

    for ip in ${NODE_IPS}; do
    ETCDCTL_API=3 /kubernetes/local/bin/etcdctl \
    --endpoints=https://${ip}:2379 \
    --cacert=/etc/kubernetes/ssl/ca.pem \
    --cert=/etc/etcd/ssl/etcd.pem \
    --key=/etc/etcd/ssl/etcd-key.pem \
    endpoint health; done

# flannel
    ifconfig flannel.1
    ping 172.30.12.0
    ping 172.30.76.0
    ping 172.30.18.0

# kube-apiserver

å·²ç»æ²¡æœ‰tokenéªŒè¯æ–¹å¼äº†ï¼Œéœ€è¦æŒ‡å®škubeletéœ€è¦çš„éªŒè¯æ–‡ä»¶
åˆ›å»ºä¸€ä¸ªencryption-config.yamlï¼Œæš‚æ—¶ä¸çŸ¥é“æœ‰å•¥ç”¨

kube-apiserverå®‰è£…æ–¹å¼æœ‰å¤šç§ï¼Œå•èŠ‚ç‚¹ï¼šæ˜“äºéƒ¨ç½²ï¼Œä½†æ˜¯å®¹é”™å·®ï¼Œä¸€ç§replicaï¼Œä¸€ç§HAï¼Œå¯¹äºäº‘ä¸Šå®‰è£…å¯ä»¥ä¸è€ƒè™‘HAç›´æ¥é‡‡ç”¨google cloudçš„æœåŠ¡æˆ–è€…AWSçš„æœåŠ¡ï¼Œå¦‚æœç‰©ç†æœºéƒ¨ç½²æ¨èé‡‡ç”¨replicaæ–¹å¼ã€‚ä½†æ˜¯éƒ¨ç½²è°ƒè¯•æ—¶å»ºè®®ä½¿ç”¨å•æœºæ¨¡å¼ã€‚

api-serveræµ‹è¯•è¿‡ç¨‹ï¼š

## api-serveré—®é¢˜è®°å½•
1ã€journalctl -xeä¸­å¦‚ä¸‹æ—¥å¿—åˆ·ä¸€å¤§å †ï¼Œè€Œä¸”æ—¶æ—¶åˆ»åˆ»åœ¨åˆ·ï¼›
```shell
    1æœˆ 07 14:40:16 cloud25 kube-apiserver[19798]: I0107 14:40:16.695016   19798 wrap.go:42] GET /apis/admissionregistration.k8s.io/v1alpha1/initializerconfigurations: (2.686028ms) 404 [[kube-apiserver/v1.10.2 (linux/amd64) kubernetes/81753b1
```
è¿™ä¸ªæ˜¯dymanic admission controlçš„é—®é¢˜,æ­¤é¡¹ç‰¹æ€§åœ¨ç›®å‰çš„ç‰ˆæœ¬ï¼ˆ1.10.2ï¼‰ä¸‹æ˜¯æµ‹è¯•ç‰¹æ€§ï¼Œé»˜è®¤å…³é—­éœ€è¦æ‰‹åŠ¨å¼€å¯ï¼Œç„¶è€Œå¼€å¯è¿™ä¸ªç‰¹æ€§éœ€è¦åœ¨å¯åŠ¨çš„æ—¶å€™åŠ å…¥ä¸¤ä¸ªå‚æ•°ï¼Œå¦‚æœå¹¶ä¸”éœ€è¦æŒ‡å®šä¸‹é¢ä¸¤é¡¹ï¼ˆenable-admission-pluginsä¸­åŠ å…¥Initializersï¼Œå¹¶ä¸”--runtime-configåŠ å…¥admissionregistration.k8s.io/v1alpha1ï¼‰ï¼š
```shell
 --enable-admission-plugins=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \
 --runtime-config=api/all,admissionregistration.k8s.io/v1alpha1 \
```

æ³¨ï¼šåœ¨å¤šæ•°æ‰‹åŠ¨å®‰è£…æ•™ç¨‹ä¸­ï¼Œ--runtime-config=api/all \ åœ¨å®é™…å®‰è£…ä¸­ï¼Œå¯ä»¥å…ˆä¸åŠ åé¢é‚£é¡¹ï¼Œå¦‚æœé‡åˆ°é—®é¢˜äº†å†åŠ ä¸Šä¹Ÿä¸éº»çƒ¦ã€‚

2ã€journalctl -xeä¸­å¦‚ä¸‹æ—¥å¿—åˆ·ä¸€å¤§å †ï¼Œè€Œä¸”æ—¶æ—¶åˆ»åˆ»åœ¨åˆ·ï¼›
```shell
    1æœˆ 07 14:40:16 cloud25 kube-apiserver[19798]: I0107 14:40:16.695016   19798 wrap.go:42] GET /apis/admissionregistration.k8s.io/v1alpha1/initializerconfigurations: (2.686028ms) 200 [[kube-apiserver/v1.10.2 (linux/amd64) kubernetes/81753b1
```
çœ‹èµ·æ¥å’Œä¸Šé¢ä¸€æ ·ï¼Œå…¶å®æ˜¯404æ¢æˆäº†200ã€‚è¿™äº›æ—¥å¿—åº”è¯¥æ²¡æœ‰å¤ªå¤šå½±å“ï¼Œåªè¦kubectlèƒ½æ­£å¸¸è·å–ç³»ç»Ÿé…ç½®å°±è¡Œã€‚ä½†æ˜¯è¿™2ç§’17æ¡å®åœ¨æ˜¯å¤ªå½±å“æ—¥å¿—å¯è¯»æ€§äº†ã€‚journalctl -u kube-apiserverä¸­çš„æ‰€æœ‰æœ‰ç”¨ä¿¡æ¯éƒ½è¢«æ·¹æ²¡åœ¨è¿™äº›ä¸œè¥¿é‡Œäº†ã€‚

3ã€ç«¯å£ä¸å¯¹ï¼ŒapiserveræŒ‡å®šçš„æ˜¯6443ç«¯å£ã€‚
çœ‹èµ·æ¥æ˜¯ä¸€ä¸ªå°é—®é¢˜ï¼Œç„¶è€Œè¿™é—®é¢˜è€½è¯¯äº†æˆ‘åŠå¤©æ—¶é—´ã€‚é¦–å…ˆè¦è¯´æ˜çš„æ˜¯ï¼Œä¸è®ºæ€ä¹ˆå®‰è£…ï¼Œä¹‹å‰éƒ½è¦åˆ›å»º~/.kube/configæ–‡ä»¶ï¼Œåœ¨åˆ›å»ºè¿™ä¸ªæ–‡ä»¶æ—¶æ˜¯é€šè¿‡kubectlè¿›è¡Œå†™å…¥çš„ï¼Œè€Œä¸”å¾€å¾€è¿™ä¸ªæ–‡ä»¶é…ç½®çš„æ¯”è¾ƒæ—©ï¼Œå®é™…é…ç½®apiserverçš„æ—¶å€™å¤§å¤šåˆä¸ä¼šæ‰‹åŠ¨æŒ‡å®šç«¯å£ï¼Œå¦‚æœé…ç½®.kube/configä¸­çš„é…ç½®æ—¶æŒ‡å®šçš„ä¸æ˜¯6443ç«¯å£ï¼Œé‚£ä¹ˆå°±éš¾å—äº†ã€‚
ç³»ç»Ÿä¼šæç¤ºï¼š
```shell
  kubectl cluster-info
  Kubernetes master is running at https://192.168.12.165:xxxx
  The connection to the server 192.168.12.165:xxxx was refused - did you specify the right host or port?
```
æˆ‘çœ‹åˆ°è¿™æ—¥å¿—çš„ç¬¬ä¸€ååº”æ˜¯è¿æ¥è¢«æ‹’ç»ï¼Œæ˜¯ä¸æ˜¯è¯ä¹¦è¿‡æœŸï¼Ÿæˆ–è€…è¯´å¯åŠ¨å‡ºé”™ï¼Ÿé…åˆä¸Šé¢æåˆ°çš„ç¬¬2ä¸ª2ç§’17æ¡çš„æ—¥å¿—æ•ˆæœæ‹”ç¾¤ã€‚æ€»ä»¥ä¸ºæ˜¯å¯åŠ¨é—®é¢˜ï¼Œå…¶å®â€œKubernetes master is running at https://192.168.12.165:xxxxâ€è¿™å¥è¯æ”¹æˆâ€œconnecting to kubernetes master: https://192.168.12.165:xxxxâ€å¯èƒ½æ¯”è¾ƒå®¹æ˜“æƒ³åˆ°æ˜¯IPæˆ–è€…ç«¯å£æŒ‡å®šçš„é—®é¢˜ã€‚æˆ‘æ˜¯æœ€åå‘ç°netstat -ano | grep 8443æ²¡å¯åŠ¨ï¼ŒæŸ¥stackoverflowæ—¶å‘ç°æœ‰äººé‡åˆ°ç±»ä¼¼çš„é—®é¢˜ï¼Œæœ€åé€šè¿‡ä¿®æ”¹./kube/configè§£å†³çš„ï¼Œæˆ‘æ‰ååº”è¿‡æ¥è¦æŸ¥ä¸€ä¸‹apiserverçš„é»˜è®¤ç«¯å£ï¼Œä¸€æŸ¥æ˜¯6443ç„¶åå‘ç°å·²ç»ç›‘å¬å¥½ä¹…äº†ã€‚

# docker
    éœ€è¦åœ¨é…ç½®é‡ŒæŒ‡å®šPATHï¼Œä¸ç„¶æ‰¾ä¸åˆ°å…¶ä»–è¿è¡Œè„šæœ¬

# kubelet
    1ã€è®°å¾—æŠŠworkspaceè·¯å¾„æå‰åˆ›å»ºå¥½

## kubeleté—®é¢˜è®°å½•
    åœ¨å¼€å§‹è¿è¡Œkubeletæ—¶ä¼šè¿›è¡Œflagsçš„æ£€æµ‹ï¼Œæ£€æµ‹ä¸€éç³»ç»Ÿå‚æ•°ä¹‹åé€šè¿‡--configæŒ‡å®šçš„configæ–‡ä»¶è¿›è¡Œä¸‹ä¸€æ­¥çš„é…ç½®è¯»å–ã€‚é‚£ä¹ˆé—®é¢˜æ¥äº†ï¼š
    flagsè‡ªæ£€é€šè¿‡äº†ä¹‹åï¼Œconfigçš„è‡ªæ£€ä¸æ˜¾ç¤ºåœ¨æ—¥å¿—ä¸­ï¼Œemmmmmmm....ä¾‹å¦‚ä¸‹é¢è¿™ä¸ªæ²™é›•æ—¥å¿—ï¼š
    journalctl -xefu kubelet

```shell
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: Flag --fail-swap-on has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.969989    2672 flags.go:27] FLAG: --address="0.0.0.0"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970212    2672 flags.go:27] FLAG: --allow-privileged="false"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970224    2672 flags.go:27] FLAG: --alsologtostderr="true"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970233    2672 flags.go:27] FLAG: --anonymous-auth="true"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970239    2672 flags.go:27] FLAG: --application-metrics-count-limit="100"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970245    2672 flags.go:27] FLAG: --authentication-token-webhook="false"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970251    2672 flags.go:27] FLAG: --authentication-token-webhook-cache-ttl="2m0s"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970261    2672 flags.go:27] FLAG: --authorization-mode="AlwaysAllow"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970269    2672 flags.go:27] FLAG: --authorization-webhook-cache-authorized-ttl="5m0s"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970276    2672 flags.go:27] FLAG: --authorization-webhook-cache-unauthorized-ttl="30s"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970281    2672 flags.go:27] FLAG: --azure-container-registry-config=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970287    2672 flags.go:27] FLAG: --boot-id-file="/proc/sys/kernel/random/boot_id"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970294    2672 flags.go:27] FLAG: --bootstrap-checkpoint-path=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970300    2672 flags.go:27] FLAG: --bootstrap-kubeconfig=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970306    2672 flags.go:27] FLAG: --cadvisor-port="4194"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970316    2672 flags.go:27] FLAG: --cert-dir="/var/lib/kubelet/pki"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970323    2672 flags.go:27] FLAG: --cgroup-driver="cgroupfs"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970329    2672 flags.go:27] FLAG: --cgroup-root=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970335    2672 flags.go:27] FLAG: --cgroups-per-qos="true"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970341    2672 flags.go:27] FLAG: --chaos-chance="0"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970351    2672 flags.go:27] FLAG: --client-ca-file=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970357    2672 flags.go:27] FLAG: --cloud-config=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970363    2672 flags.go:27] FLAG: --cloud-provider=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970368    2672 flags.go:27] FLAG: --cluster-dns="[]"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970383    2672 flags.go:27] FLAG: --cluster-domain=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970389    2672 flags.go:27] FLAG: --cni-bin-dir=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970394    2672 flags.go:27] FLAG: --cni-conf-dir=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970400    2672 flags.go:27] FLAG: --config="/kubernetes/config/kubelet.config.json"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970407    2672 flags.go:27] FLAG: --container-hints="/etc/cadvisor/container_hints.json"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970413    2672 flags.go:27] FLAG: --container-log-max-files="5"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970419    2672 flags.go:27] FLAG: --container-log-max-size="10Mi"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970425    2672 flags.go:27] FLAG: --container-runtime="docker"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970431    2672 flags.go:27] FLAG: --container-runtime-endpoint="unix:///var/run/dockershim.sock"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970437    2672 flags.go:27] FLAG: --containerd="unix:///var/run/containerd.sock"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970443    2672 flags.go:27] FLAG: --containerized="false"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970449    2672 flags.go:27] FLAG: --contention-profiling="false"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970454    2672 flags.go:27] FLAG: --cpu-cfs-quota="true"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970460    2672 flags.go:27] FLAG: --cpu-manager-policy="none"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970466    2672 flags.go:27] FLAG: --cpu-manager-reconcile-period="10s"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970471    2672 flags.go:27] FLAG: --docker="unix:///var/run/docker.sock"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970478    2672 flags.go:27] FLAG: --docker-disable-shared-pid="true"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970483    2672 flags.go:27] FLAG: --docker-endpoint="unix:///var/run/docker.sock"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970489    2672 flags.go:27] FLAG: --docker-env-metadata-whitelist=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970495    2672 flags.go:27] FLAG: --docker-only="false"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970500    2672 flags.go:27] FLAG: --docker-root="/var/lib/docker"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970506    2672 flags.go:27] FLAG: --docker-tls="false"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970512    2672 flags.go:27] FLAG: --docker-tls-ca="ca.pem"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970518    2672 flags.go:27] FLAG: --docker-tls-cert="cert.pem"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970524    2672 flags.go:27] FLAG: --docker-tls-key="key.pem"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970530    2672 flags.go:27] FLAG: --dynamic-config-dir=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970539    2672 flags.go:27] FLAG: --enable-controller-attach-detach="true"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970548    2672 flags.go:27] FLAG: --enable-custom-metrics="false"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970554    2672 flags.go:27] FLAG: --enable-debugging-handlers="true"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970560    2672 flags.go:27] FLAG: --enable-load-reader="false"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970566    2672 flags.go:27] FLAG: --enable-server="true"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970572    2672 flags.go:27] FLAG: --enforce-node-allocatable="[pods]"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970586    2672 flags.go:27] FLAG: --event-burst="10"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970592    2672 flags.go:27] FLAG: --event-qps="5"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970599    2672 flags.go:27] FLAG: --event-storage-age-limit="default=0"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970605    2672 flags.go:27] FLAG: --event-storage-event-limit="default=0"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970610    2672 flags.go:27] FLAG: --eviction-hard="imagefs.available<15%,memory.available<100Mi,nodefs.available<10%,nodefs.inodesFree<5%"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970629    2672 flags.go:27] FLAG: --eviction-max-pod-grace-period="0"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970636    2672 flags.go:27] FLAG: --eviction-minimum-reclaim=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970645    2672 flags.go:27] FLAG: --eviction-pressure-transition-period="5m0s"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970651    2672 flags.go:27] FLAG: --eviction-soft=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970657    2672 flags.go:27] FLAG: --eviction-soft-grace-period=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970664    2672 flags.go:27] FLAG: --exit-on-lock-contention="false"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970670    2672 flags.go:27] FLAG: --experimental-allocatable-ignore-eviction="false"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970676    2672 flags.go:27] FLAG: --experimental-allowed-unsafe-sysctls="[]"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970686    2672 flags.go:27] FLAG: --experimental-bootstrap-kubeconfig=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970693    2672 flags.go:27] FLAG: --experimental-check-node-capabilities-before-mount="false"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970699    2672 flags.go:27] FLAG: --experimental-dockershim="false"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970705    2672 flags.go:27] FLAG: --experimental-dockershim-root-directory="/var/lib/dockershim"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970711    2672 flags.go:27] FLAG: --experimental-fail-swap-on="false"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970716    2672 flags.go:27] FLAG: --experimental-kernel-memcg-notification="false"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970722    2672 flags.go:27] FLAG: --experimental-mounter-path=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970728    2672 flags.go:27] FLAG: --experimental-qos-reserved=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970734    2672 flags.go:27] FLAG: --fail-swap-on="false"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970740    2672 flags.go:27] FLAG: --feature-gates=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970748    2672 flags.go:27] FLAG: --file-check-frequency="20s"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970754    2672 flags.go:27] FLAG: --global-housekeeping-interval="1m0s"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970761    2672 flags.go:27] FLAG: --google-json-key=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970767    2672 flags.go:27] FLAG: --hairpin-mode="promiscuous-bridge"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970774    2672 flags.go:27] FLAG: --healthz-bind-address="127.0.0.1"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970780    2672 flags.go:27] FLAG: --healthz-port="10248"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970786    2672 flags.go:27] FLAG: --help="false"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970792    2672 flags.go:27] FLAG: --host-ipc-sources="[*]"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970805    2672 flags.go:27] FLAG: --host-network-sources="[*]"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970824    2672 flags.go:27] FLAG: --host-pid-sources="[*]"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970836    2672 flags.go:27] FLAG: --hostname-override=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970842    2672 flags.go:27] FLAG: --housekeeping-interval="10s"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970848    2672 flags.go:27] FLAG: --http-check-frequency="20s"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970854    2672 flags.go:27] FLAG: --image-gc-high-threshold="85"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970860    2672 flags.go:27] FLAG: --image-gc-low-threshold="80"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970866    2672 flags.go:27] FLAG: --image-pull-progress-deadline="2m0s"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970872    2672 flags.go:27] FLAG: --image-service-endpoint=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970877    2672 flags.go:27] FLAG: --iptables-drop-bit="15"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970883    2672 flags.go:27] FLAG: --iptables-masquerade-bit="14"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970889    2672 flags.go:27] FLAG: --keep-terminated-pod-volumes="false"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970895    2672 flags.go:27] FLAG: --kube-api-burst="10"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970900    2672 flags.go:27] FLAG: --kube-api-content-type="application/vnd.kubernetes.protobuf"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970909    2672 flags.go:27] FLAG: --kube-api-qps="5"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970915    2672 flags.go:27] FLAG: --kube-reserved=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970921    2672 flags.go:27] FLAG: --kube-reserved-cgroup=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970927    2672 flags.go:27] FLAG: --kubeconfig="/kubernetes/config/cloud25.kubeconfig"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970934    2672 flags.go:27] FLAG: --kubelet-cgroups=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970940    2672 flags.go:27] FLAG: --lock-file=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970946    2672 flags.go:27] FLAG: --log-backtrace-at=":0"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970954    2672 flags.go:27] FLAG: --log-cadvisor-usage="false"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970960    2672 flags.go:27] FLAG: --log-dir=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970966    2672 flags.go:27] FLAG: --log-flush-frequency="5s"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970973    2672 flags.go:27] FLAG: --logtostderr="false"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970979    2672 flags.go:27] FLAG: --machine-id-file="/etc/machine-id,/var/lib/dbus/machine-id"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970986    2672 flags.go:27] FLAG: --make-iptables-util-chains="true"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970992    2672 flags.go:27] FLAG: --manifest-url=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.970998    2672 flags.go:27] FLAG: --manifest-url-header=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.971008    2672 flags.go:27] FLAG: --master-service-namespace="default"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.971014    2672 flags.go:27] FLAG: --max-open-files="1000000"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.971023    2672 flags.go:27] FLAG: --max-pods="110"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.971029    2672 flags.go:27] FLAG: --maximum-dead-containers="-1"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.971035    2672 flags.go:27] FLAG: --maximum-dead-containers-per-container="1"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.971041    2672 flags.go:27] FLAG: --minimum-container-ttl-duration="0s"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.971047    2672 flags.go:27] FLAG: --minimum-image-ttl-duration="2m0s"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.971053    2672 flags.go:27] FLAG: --network-plugin=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.971059    2672 flags.go:27] FLAG: --network-plugin-mtu="0"
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.971065    2672 flags.go:27] FLAG: --node-ip=""
1æœˆ 09 17:15:49 cloud25 kubelet[2672]: I0109 17:15:49.971071    2672 flags.go:27] FLAG: --node-labels=""
1æœˆ 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971079    2672 flags.go:27] FLAG: --node-status-update-frequency="10s"
1æœˆ 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971085    2672 flags.go:27] FLAG: --non-masquerade-cidr="10.0.0.0/8"
1æœˆ 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971091    2672 flags.go:27] FLAG: --oom-score-adj="-999"
1æœˆ 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971098    2672 flags.go:27] FLAG: --pod-cidr=""
1æœˆ 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971103    2672 flags.go:27] FLAG: --pod-infra-container-image="anjia0532/pause-amd64:3.1"
1æœˆ 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971110    2672 flags.go:27] FLAG: --pod-manifest-path=""
1æœˆ 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971115    2672 flags.go:27] FLAG: --pod-max-pids="-1"
1æœˆ 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971121    2672 flags.go:27] FLAG: --pods-per-core="0"
1æœˆ 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971127    2672 flags.go:27] FLAG: --port="10250"
1æœˆ 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971133    2672 flags.go:27] FLAG: --protect-kernel-defaults="false"
1æœˆ 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971139    2672 flags.go:27] FLAG: --provider-id=""
1æœˆ 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971145    2672 flags.go:27] FLAG: --read-only-port="10255"
1æœˆ 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971151    2672 flags.go:27] FLAG: --really-crash-for-testing="false"
1æœˆ 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971156    2672 flags.go:27] FLAG: --register-node="true"
1æœˆ 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971162    2672 flags.go:27] FLAG: --register-schedulable="true"
1æœˆ 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971169    2672 flags.go:27] FLAG: --register-with-taints=""
1æœˆ 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971177    2672 flags.go:27] FLAG: --registry-burst="10"
1æœˆ 09 17:15:50 cloud25 kubelet[2672]: I0109 17:15:49.971183    2672 flags.go:27] FLAG: --registry-qps="5"
1æœˆ 09 17:15:50 cloud25 systemd[1]: kubelet.service: Main process exited, code=exited, status=255/n/a
1æœˆ 09 17:15:50 cloud25 systemd[1]: kubelet.service: Unit entered failed state.
1æœˆ 09 17:15:50 cloud25 systemd[1]: kubelet.service: Failed with result 'exit-code'.
1æœˆ 09 17:15:55 cloud25 systemd[1]: kubelet.service: Service hold-off time over, scheduling restart.
1æœˆ 09 17:15:55 cloud25 systemd[1]: Stopped Kubernetes Kubelet.
-- Subject: Unit kubelet.service has finished shutting down
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
--
-- Unit kubelet.service has finished shutting down.
1æœˆ 09 17:15:55 cloud25 systemd[1]: Started Kubernetes Kubelet.
-- Subject: Unit kubelet.service has finished start-up
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
--
-- Unit kubelet.service has finished starting up.
--
-- The start-up result is done.
1æœˆ 09 17:15:56 cloud25 kubelet[2762]: W0109 17:15:55.813586    2762 docker_sandbox.go:353] failed to read pod IP from plugin/docker: Couldn't find network status for kube-system/kube-dns-57b95f54f9-b9jwz through plugin: invalid network status for
1æœˆ 09 17:15:56 cloud25 systemd[1]: kubelet.service: Main process exited, code=exited, status=255/n/a
1æœˆ 09 17:15:56 cloud25 systemd[1]: kubelet.service: Unit entered failed state.
1æœˆ 09 17:15:56 cloud25 systemd[1]: kubelet.service: Failed with result 'exit-code'.
1æœˆ 09 17:15:56 cloud25 systemd[1]: Stopped Kubernetes Kubelet.
-- Subject: Unit kubelet.service has finished shutting down
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
--
-- Unit kubelet.service has finished shutting down.
1æœˆ 09 17:28:20 cloud25 systemd[1]: Stopped Kubernetes Kubelet.
-- Subject: Unit kubelet.service has finished shutting down
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
--
-- Unit kubelet.service has finished shutting down.
1æœˆ 09 17:28:30 cloud25 systemd[1]: Stopped Kubernetes Kubelet.
-- Subject: Unit kubelet.service has finished shutting down
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
--
-- Unit kubelet.service has finished shutting down.
```

æˆ‘åœ¨æƒ³è¿™ä¸ªæ—¥å¿—å‡ºçš„é—®é¢˜å”¯ä¸€ä¸€ä¸ªé—®é¢˜æ˜¯kube-dnsç»„ä»¶æ‰¾ä¸åˆ°ï¼Œä½†æ˜¯ä¸åº”è¯¥å¯åŠ¨ä¸èµ·æ¥ã€‚äºæ˜¯å»googleæŸ¥äº†ä¸€ä¸‹ï¼Œåœ¨githubä¸Šæœ‰ç±»ä¼¼çš„issueï¼Œä½†æ˜¯ä¸¤ä¸ªé—®é¢˜çš„è§£å†³æ–¹æ¡ˆåˆ†åˆ«æ˜¯ï¼šå…³é—­swapï¼Œæˆ‘è¿™é‡Œæ·»åŠ äº†flag: --fail-swap-on=falseï¼Œåº”è¯¥ä¸æ˜¯è¿™ä¸ªé—®é¢˜ã€‚è¿˜æœ‰ä¸€ä¸ªæ˜¯æ‰¾ä¸åˆ°ca-pemï¼Œä½†æ˜¯æˆ‘configæ–‡ä»¶æŒ‡å®šäº†ã€‚
è¯•äº†å¥½ä¹…å‘ç°æ˜¯configæ–‡ä»¶é‡Œçš„ca-key.pemå¯¹åº”çš„keyå†™é”™äº†ã€‚è¯»é…ç½®æ—¶åº”å½“æ˜¯æ²¡è¯»å‡ºæ¥ã€‚æ‰€ä»¥éƒ¨ç½²kubeletæ—¶ä¸€å®šè¦æ³¨æ„æŸ¥çœ‹è‡ªå·±çš„configæ–‡ä»¶æ˜¯å¦ä¹¦å†™æ­£ç¡®ï¼Œå°¤å…¶æ˜¯ä¸è®ºjsonè¿˜æ˜¯yamléƒ½ä¸å¤§å¥½å†™ã€‚

2ã€ä¸‹è½½ä¸ä¸‹æ¥pauseå®¹å™¨
ç”±äºpauseå®¹å™¨æ˜¯kubernetesçš„åŸºç¡€å®¹å™¨ï¼Œæ‰€ä»¥éƒ¨ç½²Kubeletæ—¶ä¼šä¸‹è½½è¿™ä¸ªå®¹å™¨ï¼Œä½†æ˜¯ç”±äºæŸäº›åŸå› ï¼Œè¿™ä¸ªç½‘ç«™å¹¶ä¸èƒ½è®¿é—®ã€‚æ‰€ä»¥éœ€è¦åœ¨kubelet.serviceä¸­åŠ ä¸€é¡¹è®¾ç½®:
```shell
 --pod-infra-container-image=anjia0532/pause-amd64:3.1 \
```
æ­¤é•œåƒæ˜¯å›½äººå†™çš„è‡ªåŠ¨ä»æŸä¸ªä¸èƒ½è®¿é—®çš„ç½‘ç«™æ‹‰å–çš„ï¼Œæ›´æ–°åŠæ—¶ï¼Œæ¯”è‡ªå·±é€šè¿‡dockerhubè½¬æ‹‰æ–¹ä¾¿çš„å¤šï¼Œè‡³å°‘çœçš„è®¾ç½®dockerhubå¤–éƒ¨é“¾æ¥é‚£äº›æ­¥éª¤ã€‚

# kube-proxy

# åŠŸèƒ½æµ‹è¯•
## é‡åˆ°çš„é—®é¢˜
1ã€å½“åˆ›å»ºå®Œå‡ ä¸ªæµ‹è¯•ç”¨podä¹‹åï¼Œä½¿ç”¨kubectl get pods è¿”å›No resources found.
å½“create -f è¿”å›createdä¹‹åï¼Œåˆ›å»ºå‘½ä»¤è¢«åˆ†å‘åˆ°controller managerè¿›è¡Œå¤„ç†ï¼Œæ­¤æ—¶å¦‚æœcontroller managerå‡ºç°ä»€ä¹ˆé—®é¢˜ï¼Œé‚£ä¹ˆå°±ä¼šå¯¼è‡´åˆ›å»ºPodå¤±è´¥ã€‚
åœ¨æœ¬æ¬¡æµ‹è¯•ä¸­ï¼Œå‡ºç°çš„é—®é¢˜æ˜¯è¿æ¥ä¸ä¸Škube-apiserverï¼Œæ£€æŸ¥kube-controller-manager.kubeconfigä¸­çš„serveræ˜¯å¦æ­£ç¡®ï¼Œè¿™ä¸ªä¾‹å­ä¸­portåº”è¯¥æ˜¯6443è€Œä¸æ˜¯8443ï¼š
```shell
journalctl -xefu kube-controller-manager
1æœˆ 10 12:54:03 cloud25 kube-controller-manager[12390]: E0110 12:54:03.903735   12390 leaderelection.go:242] error retrieving resource lock kube-system/kube-controller-manager: Get https://192.168.12.165:8443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 192.168.12.165:8443: getsockopt: connection refused
1æœˆ 10 12:54:05 cloud25 kube-controller-manager[12390]: E0110 12:54:05.972148   12390 leaderelection.go:242] error retrieving resource lock kube-system/kube-controller-manager: Get https://192.168.12.165:8443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 192.168.12.165:8443: getsockopt: connection refused
1æœˆ 10 12:54:07 cloud25 kube-controller-manager[12390]: E0110 12:54:07.980693   12390 leaderelection.go:242] error retrieving resource lock kube-system/kube-controller-manager: Get https://192.168.12.165:8443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 192.168.12.165:8443: getsockopt: connection refused
1æœˆ 10 12:54:11 cloud25 kube-controller-manager[12390]: E0110 12:54:11.739869   12390 leaderelection.go:242] error retrieving resource lock kube-system/kube-controller-manager: Get https://192.168.12.165:8443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 192.168.12.165:8443: getsockopt: connection refused
1æœˆ 10 12:54:15 cloud25 kube-controller-manager[12390]: E0110 12:54:15.174241   12390 leaderelection.go:242] error retrieving resource lock kube-system/kube-controller-manager: Get https://192.168.12.165:8443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 192.168.12.165:8443: getsockopt: connection refused
```

# kubeDNSï¼ˆdeprecated,ç›®å‰ä½¿ç”¨coreDNSï¼‰
## ä¿®æ”¹kube-dns.yaml
    #å…ˆå»githubæ‘¸ä¸ªå®Œæ•´æºç å›æ¥
    git clone https://github.com/kubernetes/kubernetes.git
    #cd kubernetes/cluster/addons/dns
    mv kube-dns.yaml.base kube-dns.yaml
    vim kube-dns.yaml
    #ä¿®æ”¹ä¸€äº›ä¹±ä¸ƒå…«ç³Ÿä¸œè¥¿
    clusterIPæ˜¯å‡†å¤‡å¥½çš„DNSåœ°å€ï¼Œimageåæ­£è¿ä¸ä¸Šï¼Œçˆ±æ”¹æˆå“ªéƒ½è¡Œï¼Œåæ­£åŸç†å°±æ˜¯æ‹‰ä¸ªé•œåƒå›æ¥ï¼Œyueyingwuhuaè¿™ä¸ªæ˜¯æˆ‘è‡ªå·±åœ¨dockerhubä¸Šæ‘¸çš„é•œåƒã€‚
    cloud@cloud25:~/k8s$ diff kube-dns.yaml.base kube-dns.yaml
    33c33
    <   clusterIP: __PILLAR__DNS__SERVER__
    ---
    >   clusterIP: 192.254.0.2
    98c98
    <         image: k8s.gcr.io/k8s-dns-kube-dns-amd64:1.14.10
    ---
    >         image: yueyingwuhua/k8s-dns-kube-dns-amd64:1.14.10
    128c128
    <         - --domain=__PILLAR__DNS__DOMAIN__.
    ---
    >         - --domain=cluster.local.
    149c149
    <         image: k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.14.10
    ---
    >         image: yueyingwuhua/k8s-dns-dnsmasq-nanny-amd64:1.14.10
    169c169
    <         - --server=/__PILLAR__DNS__DOMAIN__/127.0.0.1#10053
    ---
    >         - --server=/cluster.local/127.0.0.1#10053
    188c188
    <         image: k8s.gcr.io/k8s-dns-sidecar-amd64:1.14.10
    ---
    >         image: yueyingwuhua/k8s-dns-sidecar-amd64:1.14.10
    201,202c201,202
    <         - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.__PILLAR__DNS__DOMAIN__,5,SRV
    <         - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.__PILLAR__DNS__DOMAIN__,5,SRV
    ---
    >         - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,SRV
    >         - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,SRV


## ä½¿ç”¨dockerhubè·å–æœ€æ–°gcrä¸Šçš„é•œåƒæ•™ç¨‹

  æœ¬éƒ¨åˆ†å†…å®¹æ—¨åœ¨ä»‹ç»å¦‚ä½•é€šè¿‡dockerhubè½¬å‚¨gcr.ioä¸Šé¢çš„é•œåƒï¼Œå½“éœ€è¦ä½¿ç”¨è¿™äº›é•œåƒæ—¶å¯ä»¥é€šè¿‡dockerhubç›´æ¥ä½¿ç”¨è€Œä¸æ˜¯é€šè¿‡ç›®ç”°ä¸Šç½‘å·¥å…·ã€‚è¿™ä¸ªæ–¹æ³•çš„åŸç†æ˜¯dockerhubæ”¯æŒå¤–éƒ¨é•œåƒçš„æ„å»ºï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å†™dockerfileçš„æ–¹å¼å°†gcr.ioä¸Šé¢çš„é•œåƒç›´æ¥åŸå°ä¸åŠ¨çš„æ‹–åˆ°dockerhubä¸Šï¼Œå®ç°è½¬å‚¨ã€‚  

### åœ¨githubä¸Šåˆ›å»ºä¸€ä¸ªrepo
  ![image text](https://raw.githubusercontent.com/YueYingWuHua/Documents/master/images/download_gcr_image/repo-for-dockerhub.jpg)
  å¦‚ä¸Šå›¾æ˜¯æˆ‘åˆ›å»ºå¥½çš„ä¸€ä¸ªrepoï¼Œé‡Œé¢å·²ç»æœ‰äº†4ä¸ªæ–‡ä»¶ï¼Œè¿™4ä¸ªæ–‡ä»¶å¯¹åº”çš„é•œåƒéƒ½æ˜¯åœ¨kubernetes setupè¿‡ç¨‹ä¸­éœ€è¦æˆ–æ›¾ç»éœ€è¦ç”¨åˆ°çš„ã€‚å…¶ä¸­corednsçš„å†…å®¹ä¸ºï¼š
  ```shell
  FROM k8s.gcr.io/coredns:1.0.6
  ```
  è¯´æ˜ï¼šdockerfileä»¥å¤–éƒ¨é•œåƒ`k8s.gcr.io/coredns:1.0.6`ä½œä¸ºåŸºç¡€é•œåƒï¼Œä¹‹åä»€ä¹ˆéƒ½ä¸åšï¼Œé‚£ä¹ˆè¿™ä¸ªdockerfileç”Ÿæˆå‡ºæ¥çš„é•œåƒå°±å’ŒåŸå§‹é•œåƒå®Œå…¨ç›¸åŒã€‚

### ç™»å½•dockerhubè¿›è¡Œè‡ªåŠ¨æ„å»º
  ç™»å½•dockerhubï¼Œç‚¹å‡»repositories,åœ¨é¡µé¢ä¸­ç‚¹å‡»Create Repository:

  ![image text](https://raw.githubusercontent.com/YueYingWuHua/Documents/master/images/download_gcr_image/create_repository.jpg)

  è¾“å…¥repoåç§°å’Œæè¿°ï¼Œå…¶ä»–ä¸ç”¨å˜ï¼Œç›´æ¥ç‚¹å‡»create:

  ![image text](https://raw.githubusercontent.com/YueYingWuHua/Documents/master/images/download_gcr_image/create.jpg)

  ä¹‹åéœ€è¦è¿›è¡Œä¸€äº›è‡ªåŠ¨åŒ–éƒ¨ç½²çš„é…ç½®ï¼Œé€‰æ‹©ä½ åˆ›å»ºå¥½çš„é‚£ä¸ªç©ºrepoï¼Œåœ¨buildsä¸‹é¢ç‚¹å‡»Configure Automated Buildsï¼š

  ![image text](https://raw.githubusercontent.com/YueYingWuHua/Documents/master/images/download_gcr_image/configure_build.jpg)

  å¦‚æœä¹‹å‰æ²¡æœ‰è¿æ¥è¿‡githubï¼Œè¿™æ—¶éœ€è¦è¿æ¥ä¸€ä¸‹githubï¼Œè®©githubæˆæƒdockerhubå¯ä»¥è¯»å–æ–‡ä»¶å³å¯ã€‚
  ç°åœ¨æœ€ä¸Šæ–¹é€‰æ‹©dockerfileæ‰€åœ¨çš„repoï¼Œç„¶ååœ¨åº•ä¸‹build rulesé‡Œé¢å¡«å†™docker tagå’Œdockerfile locationï¼Œç‚¹å‡»save and buildã€‚

  ![image text](https://raw.githubusercontent.com/YueYingWuHua/Documents/master/images/download_gcr_image/build_setting.jpg)

  ç­‰å‡ åˆ†é’Ÿï¼Œåˆ·æ–°ä¸€ä¸‹åº”è¯¥å°±å¯ç”¨äº†ã€‚æ¥ä¸‹æ¥å°±æ˜¯ä¼ ç»Ÿçš„ï¼š
  ```
  sudo docker pull yueyingwuhua/coredns:1.0.6
  sudo docker tag yueyingwuhua/coredns:1.0.6 k8s.gcr.io/coredns:1.0.6
  å¦‚æœä¸æƒ³æ”¹tagæˆ–è€…ä»¥åè¿˜éœ€è¦å‡çº§æ’ä»¶ï¼Œå»ºè®®ç›´æ¥ä¿®æ”¹æ’ä»¶yamlå†…å®¹
  vim coredns.yaml
  cloud@cloud25:/kubernetes/componentyaml$ diff coredns.yaml ~/kubernetes/kubernetes-1.10.12/cluster/addons/dns/coredns.yaml.base
  61c61
  <         kubernetes guinai.cloudk8s in-addr.arpa ip6.arpa {
  ---
  >         kubernetes __PILLAR__DNS__DOMAIN__ in-addr.arpa ip6.arpa {
  103c103
  <         image: yueyingwuhua/coredns:1.0.6
  ---
  >         image: k8s.gcr.io/coredns:1.0.6
  153c153
  <   clusterIP: 192.254.0.2
  ---
  >   clusterIP: __PILLAR__DNS__SERVER__
  ```

## ä½¿ç”¨é˜¿é‡Œäº‘åŠ é€ŸæœåŠ¡åŠ é€Ÿdockerhubä¸Šçš„é•œåƒè·å–
    sudo mkdir -p /etc/docker
    sudo tee /etc/docker/daemon.json <<-'EOF'
    {
        "registry-mirrors": ["https://52fb9b02.mirror.aliyuncs.com"]
    }
    EOF
    sudo systemctl daemon-reload
    sudo systemctl restart docker

## è·å–dockerhubé•œåƒ
    æ‰€æœ‰èŠ‚ç‚¹éƒ½éœ€è¦pauseå®¹å™¨
    sudo /kubernetes/local/bin/docker pull anjia0532/pause-amd64:3.1
    sudo /kubernetes/local/bin/docker images
    sudo /kubernetes/local/bin/docker tag anjia0532/pause-amd64:3.1 k8s.gcr.io/pause-amd64:3.1


## çœ‹çœ‹DNSè£…æ²¡è£…å¥½
    #å®‰è£…
    kubectl create -f kube-dns.yaml

    kubectl get pods -l k8s-app=kube-dns -n kube-system
    åº”è¯¥è¾“å‡ºå¦‚ä¸‹
    NAME                        READY     STATUS    RESTARTS   AGE
    kube-dns-57b95f54f9-b9jwz   3/3       Running   0          27s

    å¦‚æœREADYæ˜¯0/3ï¼Œé‚£ä¹ˆç”¨ä¸‹é¢å‘½ä»¤æŸ¥çœ‹é”™è¯¯ä¿¡æ¯
    kubectl describe pods -l k8s-app=kube-dns -n kube-system

    å¯ä»¥çœ‹åˆ°ï¼Œç»å¤§éƒ¨åˆ†æƒ…å†µæ˜¯å› ä¸ºk8s.gcr.ioè¿ä¸ä¸Šã€‚ï¼ˆè‡³äºä¸ºä»€ä¹ˆè¿ä¸ä¸Šï¼Œemmmm....ï¼‰
    å½“ç„¶ï¼Œä¹Ÿå¯èƒ½æ˜¯å› ä¸ºdockerhubè¿ä¸ä¸Šï¼Œdockerhubæ‰˜ç®¡åœ¨AWSä¸Šçš„ï¼ˆè‡³äºä¸ºä»€ä¹ˆè¿ä¸ä¸Šï¼Œemm....ï¼‰

    #æŠŠæ‰€æœ‰é•œåƒæ‰‹åŠ¨æ‹‰åˆ°æœ¬åœ°ï¼Œç„¶åé‡æ–°å®‰è£…,ä¸é‡è£…çš„è¯ä¼šä¸€ç›´å°è¯•æ‹‰å–é•œåƒç”¨ä¸äº†
    kubectl delete -f kube-dns.yaml
    kubectl create -f kube-dns.yaml

## éªŒè¯
    èŠ‚é€‰è‡ªkubenetes-the-hard-wayï¼Œæ­¤æ–¹æ³•çš„åŸç†æ˜¯åˆ›å»ºä¸€ä¸ªbusyboxï¼Œ3600ç§’åè‡ªåŠ¨é€€å‡ºï¼Œä½¿ç”¨kubectl execåœ¨busyboxé‡ŒæŸ¥çœ‹dnsè§£ææ˜¯å¦æœ‰é—®é¢˜ï¼Œå¦‚æœæ˜¾ç¤ºå‡ºæ¥kubernetesçš„è§£æåœ°å€å°±æ˜¯æ­£ç¡®å®‰è£…äº†dnsæœåŠ¡
    #Create a busybox deployment:

    kubectl run busybox --image=busybox --command -- sleep 3600
    #List the pod created by the busybox deployment:

    kubectl get pods -l run=busybox
    #output

    NAME                       READY     STATUS    RESTARTS   AGE
    busybox-2125412808-mt2vb   1/1       Running   0          15s
    Retrieve the full name of the busybox pod:

    POD_NAME=$(kubectl get pods -l run=busybox -o jsonpath="{.items[0].metadata.name}")
    #Execute a DNS lookup for the kubernetes service inside the busybox pod:

    kubectl exec -ti $POD_NAME -- nslookup kubernetes
    #output

    Server:    10.32.0.10
    Address 1: 10.32.0.10 kube-dns.kube-system.svc.cluster.local

    Name:      kubernetes
    Address 1: 10.32.0.1 kubernetes.default.svc.cluster.local

    é”™è¯¯æƒ…å†µï¼š
    nslookup: can't resolve 'kubernetes'
    command terminated with exit code 1
    æ­¤é”™è¯¯å‡ºç°è¯æ˜kubednsé…ç½®æœ‰é—®é¢˜ï¼Œæ¥ä¸‹æ¥å»å‘ç°é—®é¢˜ï¼š
    #åˆ›å»ºä¸€ä¸ªnginx podï¼š
    cat > pod-nginx.yaml<<EOF
    apiVersion: v1
    kind: Pod
    metadata:
      name: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
    EOF

    #è¿›å…¥podæŸ¥çœ‹resolv.conf
    kubectl exec nginx -i -t -- /bin/bash
    root@nginx:/# cat /etc/resolv.conf

    #output
    nameserver 192.254.0.2
    search default.svc.cluster.local svc.cluster.local cluster.local
    options ndots:5

    æ£€æŸ¥nameserverçš„ipåœ°å€æ˜¯å¦æ­£ç¡®ï¼›
    æ£€æŸ¥åŸŸåæ˜¯å¦æ­£ç¡®ï¼Œå¦‚ä¸Šè¿°è¾“å‡ºçš„åŸŸåæ˜¯cluster.localï¼Œéœ€è¦æ£€æŸ¥ï¼š
        #kubeletè®¾ç½®
        cat kubelet.service
        #kubeletçš„è®¤è¯,sansé‡Œæ˜¯å¦æœ‰kubernetes.default.svc.cluster.local
        cfssl-certinfo -cert kubernetes.pem
        #kube-dnsçš„é…ç½®kube-dns.yamlè¦æŠŠæ‰€æœ‰__PILLAR__DNS__DOMAIN__éƒ½æ”¹æˆcluster.local
        cat kube-dns.yaml

# coreDNS

## å®‰è£…
## æµ‹è¯•
## å‘ç°é—®é¢˜
## è§£å†³æ–¹æ¡ˆ
 referenceï¼šhttps://github.com/kubernetes/kubernetes/issues/66924
 ç»è¿‡ä¸€è½®æ’æŸ¥å‘ç°æ˜¯ImagePullBackOff,ä½ å¢™ä½ ğŸ´å‘¢ï¼Ÿ


# kube-dashboard

## å®‰è£…æ’ä»¶
    wget https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
    kubectl create -f kubernetes-dashboard.yaml
    #çœ‹çœ‹å®‰ä¸Šæ²¡æœ‰
    kubectl get pods -n kube-system
    #çœ‹çœ‹service
    kubectl get svc -n kube-system
    #çœ‹çœ‹æ—¥å¿—ï¼Œæ²¡å•¥å¤§é”™å°±è¡Œï¼Œå°‘ä¸ªheapesteræ’ä»¶æ˜¯æ­£å¸¸çš„
    kubectl logs kubernetes-dashboard-7c458d8866-gdrkr -n kube-system

## è®¿é—®
    kubectl -n kube-system edit service kubernetes-dashboard
    # Please edit the object below. Lines beginning with a '#' will be ignored,
    # and an empty file will abort the edit. If an error occurs while saving this file will be
    # reopened with the relevant failures.
    #
    apiVersion: v1
    ...
      name: kubernetes-dashboard
      namespace: kube-system
      resourceVersion: "343478"
      selfLink: /api/v1/namespaces/kube-system/services/kubernetes-dashboard-head
      uid: 8e48f478-993d-11e7-87e0-901b0e532516
    spec:
      clusterIP: 10.100.124.90
      externalTrafficPolicy: Cluster
      ports:
      - port: 443
        protocol: TCP
        targetPort: 8443
      selector:
        k8s-app: kubernetes-dashboard
      sessionAffinity: None
      type: ClusterIP
    status:
      loadBalancer: {}

    æŠŠè¿™è¡Œçš„clusterIPæ”¹æˆNodePortæŒ‰ç†è¯´å°±èƒ½è®¿é—®äº†

    kubectl get svc -n kube-system
    è¾“å‡ºï¼š
    NAME                   TYPE        CLUSTER-IP        EXTERNAL-IP   PORT(S)         AGE
    heapster               ClusterIP   192.254.138.19    <none>        80/TCP          55m
    kube-dns               ClusterIP   192.254.0.2       <none>        53/UDP,53/TCP   7h
    kubernetes-dashboard   NodePort    192.254.167.112   <none>        443:30759/TCP   42m
    monitoring-grafana     ClusterIP   192.254.248.204   <none>        80/TCP          57m
    monitoring-influxdb    ClusterIP   192.254.85.89     <none>        8086/TCP        57m

    æŸ¥çœ‹dashboardé‚£é¡¹ï¼Œå‘ç°æ˜ å°„åˆ°äº†30759ä¸Šäº†ï¼Œä½¿ç”¨https://master:portå°±èƒ½è®¿é—®äº†ï¼Œç†è®ºä¸Šæ˜¯ã€‚

## å‘ç°é—®é¢˜
    æˆ‘å‘ç°è¿™ä¸ªdashboardçš„ç«¯å£æ˜ å°„åœ¨äº†nodeçš„30759ä¸Šï¼Œäºæ˜¯åœ¨å…¶ä»–èŠ‚ç‚¹ä½¿ç”¨IP:30759è¿›è¡Œè®¿é—®ã€‚åœ¨dashboardç‰ˆæœ¬1.8ä»¥ä¸Šæ—¶ï¼Œå·²ç»ä¸å…è®¸è¿›è¡Œhttpè®¿é—®ï¼ŒåŸæ–‡å¦‚ä¸‹ï¼š
    NOTE: Dashboard should not be exposed publicly over HTTP. For domains accessed over HTTP it will not be possible to sign in. Nothing will happen after clicking Sign in button on login page.
    æˆ‘åœ¨ä¹‹å‰çš„è®¾ç½®ä¸­ï¼Œå·²ç»ç¦æ­¢äº†åŒ¿åè®¿é—®ï¼Œä¹Ÿç¬¦åˆæ­¤é¡¹è¦æ±‚ã€‚äºæ˜¯ä½¿ç”¨https://IP:portè¿›è¡Œè®¿é—®ï¼Œè¿”å›å¦‚ä¸‹ç»“æœï¼š
        æ‚¨çš„è¿æ¥ä¸æ˜¯ç§å¯†è¿æ¥
    å¹¶ä¸”è¿™ä¸ªé¡µé¢çš„é«˜çº§é‡Œæ²¡æœ‰ç»§ç»­è®¿é—®çš„æŒ‰é’®ã€‚é‚£ä¹ˆæ—¢ç„¶nodeportæ–¹æ³•æ— æ³•è®¿é—®ï¼ŒAPIserverèƒ½ä¸èƒ½è®¿é—®å‘¢ï¼Ÿæ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š
    kubectl cluster-info
    åªæœ‰å¦‚ä¸‹ä¸¤æ¡è®°å½•
    Kubernetes master is running at https://192.168.12.165:6443
    KubeDNS is running at https://192.168.12.165:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

    ä¹Ÿå°±æ˜¯è¯´å¹¶æ²¡æœ‰æŠŠdashboradå‘å¸ƒæˆserviceï¼Œæ²¡åŠæ³•é€šè¿‡apiserverçš„æ–¹å¼è®¿é—®ã€‚

## å¤§è‡´è§£å†³é—®é¢˜
    æƒ³åˆ°wgetå›æ¥çš„yamlæ–‡ä»¶æ˜¯dashboardçš„æœ€ç®€é…ç½®ï¼Œäºæ˜¯ä¾¿å°è¯•ä½¿ç”¨kuberenetesæºç ä¸­çš„addonsé‡Œé¢çš„yamlè¿›è¡Œåˆ›å»ºã€‚
    cd kubernetes/cluster/addons/dashboard/
    vim dashboard.service
    åœ¨æœ€ä¸‹é¢åŠ ä¸Šï¼š
        type: NodePort
    kubectl create -f .
    kubectl get svc -n kube-system
    æ‰¾åˆ°æ˜ å°„ç«¯å£ï¼Œç”¨httpsè®¿é—®ï¼Œå‘ç°ä¾æ—§æ˜¯ä¸æ˜¯ç§å¯†è¿æ¥ï¼Œemmmmm...
    kubectl cluster-info
    kubernetes-dashboard is running at https://192.168.12.165:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy

    å‘ç°å·²ç»è¢«å‘å¸ƒæˆä¸ºä¸€ä¸ªserviceäº†ã€‚

## APIserverè®¿é—®æ–¹å¼
    å‰æ–‡æåˆ°äº†apiserveré…ç½®ä¸­å·²ç»ç¦æ­¢äº†åŒ¿åè®¿é—®ï¼Œæ‰€ä»¥ç›´æ¥è®¿é—®serviceæ˜¯æ— æ³•ç™»å½•çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦ç”Ÿæˆä¸€ä¸ªè¯ä¹¦å¯¼å…¥åˆ°è®¿é—®ç³»ç»Ÿçš„æµè§ˆå™¨é‡Œï¼Œ
    openssl pkcs12 -export -in admin.pem -inkey admin-key.pem -out kube-admin.p12
    å°†kube-admin.p12å¯¼å…¥åˆ°æµè§ˆå™¨ä¸­å³å¯ï¼š
    chromeä¸‹æ˜¯è®¾ç½®->æœç´¢ssl->ç®¡ç†è¯ä¹¦ï¼Œç„¶åç…§ç€è¯´æ˜ç›´æ¥å¯¼å…¥ã€‚

    è®¿é—®https://192.168.12.165:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy
    é€‰æ‹©è¯ä¹¦ä¹‹åå‘ç°ä¾æ—§ä¸æ˜¯ç§å¯†è¿æ¥ï¼Œä½†è¿™æ¬¡é«˜çº§é‡Œå¯ä»¥ç»§ç»­è®¿é—®äº†....
    ç™»å½•æ–¹å¼å¦‚æœç…§ç€kubernetes-the-hard-wayæˆ–è€…opsnullæ•™ç¨‹æ­å»ºçš„å¯ä»¥é€‰æ‹©tokenæ–¹å¼ç™»å½•ã€‚

    åˆ›å»ºä¸€ä¸ªadmin-user.yamlæ–‡ä»¶ã€‚å…ˆæä¸€ä¸ªadmin-userçš„ç”¨æˆ·ï¼Œç„¶åæŠŠå®ƒç»‘åœ¨cluster-adminä¸‹ã€‚
    cat admin-user.yaml
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: admin-user
      namespace: kube-system
    ---
    apiVersion: rbac.authorization.k8s.io/v1beta1
    kind: ClusterRoleBinding
    metadata:
      name: admin-user
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: cluster-admin
    subjects:
    - kind: ServiceAccount
      name: admin-user
      namespace: kube-system

    kubectl create -f admin-user.yaml
    kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')

    åº”å½“è¾“å‡ºå¦‚ä¸‹ä¿¡æ¯ï¼š
    Name:         admin-user-token-6gl6l
    Namespace:    kube-system
    Labels:       <none>
    Annotations:  kubernetes.io/service-account.name=admin-user
                  kubernetes.io/service-account.uid=b16afba9-dfec-11e7-bbb9-901b0e532516

    Type:  kubernetes.io/service-account-token

    Data
    ====
    ca.crt:     1025 bytes
    namespace:  11 bytes
    token:      eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLTZnbDZsIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJiMTZhZmJhOS1kZmVjLTExZTctYmJiOS05MDFiMGU1MzI1MTYiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.M70CU3lbu3PP4OjhFms8PVL5pQKj-jj4RNSLA4YmQfTXpPUuxqXjiTf094_Rzr0fgN_IVX6gC4fiNUL5ynx9KU-lkPfk0HnX8scxfJNzypL039mpGt0bbe1IXKSIRaq_9VW59Xz-yBUhycYcKPO9RM2Qa1Ax29nqNVko4vLn1_1wPqJ6XSq3GYI8anTzV8Fku4jasUwjrws6Cn6_sPEGmL54sq5R4Z5afUtv-mItTmqZZdxnkRqcJLlg2Y8WbCPogErbsaCDJoABQ7ppaqHetwfM_0yMun6ABOQbIwwl8pspJhpplKwyo700OSpvTT9zlBsu-b35lzXGBRHzv5g_RA

    æŠŠtokenå¤åˆ¶ä¸‹æ¥ç™»å½•å³å¯ã€‚

## é—ç•™çš„é—®é¢˜
    NodePortè®¿é—®è¿˜æ˜¯ä¸è¡Œï¼Œç©¶ç«Ÿåº”è¯¥å¯¼å…¥ä»€ä¹ˆè¯ä¹¦æ‰èƒ½è®©chromeè®¤ä¸ºå®ƒæ˜¯å®‰å…¨çš„ã€‚


# heapsterã€grafanaã€influxdb
    è¿™ä¸‰ä¸ªä¸€èµ·è£…ã€‚
    https://github.com/kubernetes/heapster/releasesä¸‹è½½
    tar -zxvf heapster-1.5.3
    cd heapster-1.5.3/deploy/kube-config/influxdb

## ä¿®æ”¹é…ç½®æ–‡ä»¶

    # heapster.yaml
    éœ€è¦å°†heapsterå’Œsystem:heapsterç»‘å®šï¼ŒåŠ å…¥å¦‚ä¸‹æ®µ
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: heapster
    subjects:
      - kind: ServiceAccount
        name: heapster
        namespace: kube-system
    roleRef:
      kind: ClusterRole
      name: system:heapster
      apiGroup: rbac.authorization.k8s.io
    å°†gcr.io/kubernetesé“¾æ¥æ”¹æˆanjia0532ï¼ˆæ³¨ï¼šä¸€å¼€å§‹æˆ‘é€šè¿‡dockerhubæŠŠgcrçš„imageæ‹‰ä¸‹æ¥ï¼Œåæ¥å‘ç°æœ‰äººå¹²è¿™äº‹äº†è€Œä¸”ä¸€ç›´åœ¨ç»´æŠ¤å°±ç›´æ¥ç”¨äº†ï¼Œè‡ªå·±æ‹‰æ¯•ç«Ÿæ€ªéº»çƒ¦çš„ï¼Œæ¯ä¸ªimageéƒ½è¦åˆ›å»ºä¸€ä¸ªdockerfileå¹¶pushåˆ°githubä¸Šï¼Œç„¶åè¿˜å¾—ä¸€é¡¿æ“ä½œï¼Œå¦‚æœä»¥åanjia0532æ²¡æœ‰å¯¹åº”ç‰ˆæœ¬ï¼Œå»ºè®®è¿˜æ˜¯è‡ªå·±ç”¨dockerhubæ‹‰é•œåƒï¼Œæˆ–è€…ç”¨æŸäº›ç§‘å­¦çš„æ–¹æ³•è¿æ¥googleï¼‰
    æ³¨ï¼šç°åœ¨å…¨è‡ªå·±æ‹‰å–äº†ï¼Œæ•…å…¨éƒ¨æ”¹ä¸ºyueyingwuhuaäº†

    # influxdb.yaml
    image: yueyingwuhua/heapster-influxdb-amd64:v1.3.3

    # grafana.yaml
    image: yueyingwuhua/heapster-grafana-amd64:v4.4.3

## å¯åŠ¨
    kubectl create -f .

## éªŒè¯
    kubectl get svc -n kube-system
    # output

    NAME                   TYPE        CLUSTER-IP        EXTERNAL-IP   PORT(S)         AGE
    heapster               ClusterIP   192.254.138.19    <none>        80/TCP          1d
    kube-dns               ClusterIP   192.254.0.2       <none>        53/UDP,53/TCP   2d
    kubernetes-dashboard   NodePort    192.254.167.112   <none>        443:30759/TCP   1d
    monitoring-grafana     ClusterIP   192.254.248.204   <none>        80/TCP          1d
    monitoring-influxdb    ClusterIP   192.254.85.89     <none>        8086/TCP        1d

    kubectl get pods -n kube-system
    # output

    NAME                                    READY     STATUS    RESTARTS   AGE
    heapster-d59d66579-ptwj6                1/1       Running   0          1d
    kube-dns-57b95f54f9-b9jwz               3/3       Running   0          2d
    kubernetes-dashboard-5bc57d65cf-n8wrm   1/1       Running   0          1d
    monitoring-grafana-bff95c48c-w4k8z      1/1       Running   0          1d
    monitoring-influxdb-5d474bf6d5-dhspk    1/1       Running   0          1d

    kubectl logs monitoring-influxdb-5d474bf6d5-dhspk -n kube-system
    kubectl logs monitoring-grafana-bff95c48c-w4k8z -n kube-system
    kubectl logs heapster-d59d66579-ptwj6 -n kube-system

    éƒ½æ²¡å•¥é”™å°±è¡Œäº†ï¼Œå¦‚æœè¿˜æƒ³ç»§ç»­éªŒè¯å»ºè®®è®¿é—®
    kubectl cluster-info
    https://192.168.12.165:6443/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy

## é‡åˆ°çš„é—®é¢˜
### 1.10.12ä¸­heapsterè·å–çŠ¶æ€ä¿¡æ¯è¢«æ‹’ç»
æ­¤é—®é¢˜çš„å¤–åœ¨è¡¨ç°æ˜¯grafanaè·å–ä¸åˆ°æ•°æ®ï¼Œæ˜¾ç¤ºå…¨éƒ¨ä¸ºç©ºï¼Œdashboardæ²¡æœ‰ä»»ä½•èµ„æºç›‘æ§ç›¸å…³çš„å†…å®¹ã€‚
```shell
> kubectl get pods -n kube-system
> kubectl logs heapster-xxxxxx-xxxxxxxx -n kube-system
 å‘ç°å¦‚ä¸‹é—®é¢˜ï¼š

 E0116 02:34:09.090864       1 reflector.go:190] k8s.io/heapster/metrics/heapster.go:328: Failed to list *v1.Pod: pods is forbidden: User "system:serviceaccount:kube-system:heapster" cannot list pods at the cluster scope
```
ä»æ—¥å¿—æˆ‘ä»¬å¯ä»¥è·å–åˆ°çš„ä¿¡æ¯æ˜¯ï¼Œæˆ‘ä»¬çš„serviceaccountï¼škube-system:heapsteræ²¡æœ‰è¶³å¤Ÿçš„æƒé™è·å–podsä¿¡æ¯ï¼Œé‚£ä¹ˆå¯ä»¥ç¡®å®šçš„æ˜¯è§’è‰²æˆæƒå‡ºäº†é—®é¢˜ã€‚
```shell
> kubectl get clusterrolebinding
å‘ç°æœ‰å¦‚ä¸‹é¡¹ï¼š
heapster                                               3h
ï¼ˆå¦‚æœæ²¡æœ‰è¯·æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤:
> cd ~/kubernetes/kubernetes-1.10.12/cluster/addons/heapster-1.5.4/deploy/kube-config/influxdb
> cd ../rbac
> kubectl create -f heapster-rbac.yamlï¼‰

çœ‹åˆ°æˆ‘ä»¬ç¡®å®æœ‰ä¸€ä¸ªroleä¸æˆ‘ä»¬çš„serviceaccountï¼Œå³heapsterè¿›è¡Œç»‘å®šäº†ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦æŸ¥çœ‹ä¸ä¹‹ç»‘å®šçš„roleæƒé™æ˜¯å¦æ­£ç¡®ã€‚
> kubectl get clusterrolebindings heapster -o yaml
æ˜¾ç¤ºå¦‚ä¸‹ï¼ˆè¿™æ¬¡æ˜¯ç»è¿‡ä¿®æ”¹åçš„æˆæƒï¼Œå¦‚æœæ˜¯é”™è¯¯çš„è¯·æŸ¥çœ‹roleRefä¸‹çš„nameé¡¹æ˜¯å¦ä¸ºå¯¹åº”çš„è§’è‰²ï¼‰ï¼š
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  creationTimestamp: 2019-01-16T02:56:02Z
  name: heapster
  resourceVersion: "1121099"
  selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/heapster
  uid: 4272aa37-193a-11e9-9f2c-6c92bf20cf87
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:heapster
subjects:
- kind: ServiceAccount
  name: heapster
  namespace: kube-system
> kubectl describe clusterrole system:heapster
Name:         system:heapster
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
PolicyRule:
  Resources               Non-Resource URLs  Resource Names  Verbs
  ---------               -----------------  --------------  -----
  events                  []                 []              [get list watch]
  namespaces              []                 []              [get list watch]
  nodes                   []                 []              [get list watch]
  pods                    []                 []              [get list watch]
  deployments.extensions  []                 []              [get list watch]
```
è¿™æ—¶ï¼Œæˆ‘ä»¬ç¡®è®¤äº†heapsterå¯¹åº”çš„è§’è‰²ï¼Œå¦‚æœä¸å¯¹ï¼Œæ‰§è¡Œå¦‚ä¸‹å‘½ä»¤è¿›è¡Œé‡æ–°è®¾ç½®ï¼š
```shell
> cd ~/kubernetes/kubernetes-1.10.12/cluster/addons/heapster-1.5.4/deploy/kube-config/influxdb
> cd ../rbac
> kubectl delete -f heapster-rbac.yaml
> vim heapster-rbac.yaml
> kubectl create -f heapster-rbac.yaml
```
è¿™ç§è®¾ç½®æ–¹å¼åœ¨kubernetes1.10.10ä¹‹å‰ä¸ä¼šå‡ºç°å…¶ä»–é—®é¢˜ï¼Œå¦‚æœéœ€è¦åœ¨æ–°ç‰ˆæœ¬éƒ¨ç½²heapsterï¼Œå¯èƒ½é‡åˆ°ä¸‹é¢æ‰€å†™æ˜çš„é”™è¯¯ã€‚

æ¥ä¸‹æ¥æ˜¯æ–°ç‰ˆæœ¬é‡åˆ°çš„é—®é¢˜ï¼š
ç”±äºåœ¨æ–°ç‰ˆæœ¬(1.10.10+)ä¸­ï¼Œheapster+grafana+influxdbä¸å†ä½œä¸ºæ¨èçš„é›†ç¾¤ç›‘æ§æ’ä»¶äº† [^1]ï¼Œæ­¤åŠŸèƒ½ç”±Metric-serverä»£æ›¿ï¼Œå¹¶ä¸”heapsterä¹Ÿè¢«æ ‡è®°ä¸ºretiredå¹¶ä¸å†è¿›è¡Œç»´æŠ¤ï¼Œæ–°ç‰ˆæœ¬åœ¨éƒ¨ç½²heapsteræ—¶å¯èƒ½ä¼šé‡åˆ°è¶Šæ¥è¶Šå¤šçš„é—®é¢˜ï¼Œåœ¨å½“å‰ç‰ˆæœ¬ï¼ˆ1.13.2, 1.12.4, 1.11.6 ä»¥åŠæœ¬æ–‡æ¶‰åŠçš„1.10.12ï¼‰ä¸­ï¼Œéƒ¨ç½²æ—¶ä¼šé‡åˆ°å¦‚ä¸‹é—®é¢˜ï¼š
```shell
> kubectl logs heapster-xxxxxx-xxxxxxxx -n kube-system
E0116 02:42:05.040909       1 manager.go:101] Error in scraping containers from kubelet:192.168.12.165:10250: failed to get all container stats from Kubelet URL "https://192.168.12.165:10250/stats/container/": request failed - "403 Forbidden", response: "Forbidden (user=system:serviceaccount:kube-system:heapster, verb=create, resource=nodes, subresource=stats)"
```
æ­¤æ—¥å¿—ä¸»è¦è¯´æ˜äº†ä¸ºä»€ä¹ˆheapsterå¤±æ•ˆï¼ŒåŸå› æ˜¯kube-system:heapsterçš„serviceaccountå¯¹äºèµ„æºï¼šnodesçš„å­èµ„æºstatsä¸æ”¯æŒcreateæ“ä½œï¼Œæš‚ä¸”ä¸è®ºæ–°ç‰ˆæœ¬å¯¹äºæƒé™ç®¡ç†è¿›è¡Œäº†å¤šå°‘ç»†åŒ–æ“ä½œï¼Œæˆ‘ä»¬è¦åšçš„å…¶å®å°±æ˜¯æƒ³åŠæ³•å°†è¿™ä¸ªèµ„æºçš„createæ“ä½œèµ‹äºˆæˆ‘ä»¬çš„accountå³å¯ã€‚
å›åˆ°ä¸Šæ–‡ï¼Œä¹‹å‰çš„å‘½ä»¤ä¸­ï¼Œæˆ‘ä»¬æ‰§è¡Œ`kubectl describe clusterrole system:heapster`å·²ç»è·å–äº†heapsteræ‰€æœ‰æƒé™äº†ï¼Œç¡®å®æ²¡æœ‰å¯¹åº”çš„æ“ä½œï¼Œè¿™å°±å¿…é¡»æ‰‹åŠ¨åˆ›å»ºä¸€ä¸ªè§’è‰²ï¼Œå¹¶æŠŠè§’è‰²å’Œserviceaccountç»‘å®šï¼Œæ¥è·å–è¶³å¤Ÿå¤šçš„æƒé™ã€‚é¦–å…ˆåˆ›å»ºä¸€ä¸ªåŒ…å«heapsteræ‰€æœ‰æƒé™çš„roleï¼š
```yaml
> vim heapster-custom-fix.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: heapster-custom-fix
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - nodes
  - namespaces
  - events
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - extensions
  resources:
  - deployments
  verbs:
  - get
  - list
  - update
  - watch
- apiGroups:
  - ""
  resources:
  - nodes/stats
  verbs:
  - get
  - create

> kubectl create -f heapster-custom-fix.yaml
> vim heapster-rbac.yaml
æŠŠsystem:heapsterä¿®æ”¹ä¸ºè‡ªå®šä¹‰è§’è‰²heapster-custom-fix
æœ€ç»ˆæ–‡ä»¶ä¸ºï¼š
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: heapster
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: heapster-custom-fix
subjects:
- kind: ServiceAccount
  name: heapster
  namespace: kube-system

> kubectl create -f heapster-rbac.yaml
```
ä¿®æ”¹å®Œä¹‹åè®°å¾—delete -f + create -f heapster.yamlï¼Œé‡è£…ä¸€ä¸‹heapsteræ’ä»¶ä½¿æˆ‘ä»¬çš„æ’ä»¶ç”Ÿæ•ˆã€‚

## grafanaä¸èƒ½è¿æ¥æœ¬åœ°IP
å¤–åœ¨è¡¨ç°ï¼šè¿›grafanaé¡µé¢åæ‰¾ä¸åˆ°dashboardï¼Œæç¤ºéœ€è¦æ‰‹åŠ¨åˆ›å»ºã€‚
ä½¿ç”¨`kubectl logs podname -n kube-system`è·å–æ—¥å¿—ï¼š
```shell
Can't access the Grafana dashboard. Error: Get http://admin:admin@localhost:3000/api/org: dial tcp 127.0.0.1:3000: getsockopt: connection refused. Retrying after 5 seconds...
```
å‘ç°è¿æ¥ä¸ä¸Šï¼Œæ­¤é—®é¢˜æ²¡æœ‰ä»€ä¹ˆå½»åº•çš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨æœ¬æ¬¡è§£å†³ä¸­ï¼Œgrafana.yamlä¸­æŠŠtype:NodePorté‡æ–°æ³¨é‡Šæ‰å°±å¥½äº†ï¼Œç›¸å½“äºåªä¿®æ”¹äº†containerçš„åœ°å€ï¼Œå…¶ä»–éƒ½ç”¨çš„é»˜è®¤è®¾ç½®ã€‚

[^1]: references:
https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/
https://github.com/Azure/aks-engine/issues/73
https://github.com/kubernetes-retired/heapster/issues/1936

# è§£å†³ç½‘ç»œè®¿é—®ä¸é€šçš„é—®é¢˜
    çœ‹node IPï¼Œpingä¸€ä¸‹å’Œcurlä¸€ä¸‹
    iptables -P FORWARD ACCEPT
